{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2702213,"sourceType":"datasetVersion","datasetId":1645904},{"sourceId":2702226,"sourceType":"datasetVersion","datasetId":1269900},{"sourceId":5137200,"sourceType":"datasetVersion","datasetId":2984453},{"sourceId":5175158,"sourceType":"datasetVersion","datasetId":3008205}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Download Packages","metadata":{"id":"x9LNzEYERaH2"}},{"cell_type":"code","source":"!nvidia-smi -L","metadata":{"execution":{"iopub.status.busy":"2024-12-06T20:56:29.227095Z","iopub.execute_input":"2024-12-06T20:56:29.227430Z","iopub.status.idle":"2024-12-06T20:56:30.278108Z","shell.execute_reply.started":"2024-12-06T20:56:29.227397Z","shell.execute_reply":"2024-12-06T20:56:30.276247Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%capture\n!pip install -U git+https://github.com/UN-GCPDS/python-gcpds.databases #Package for database reading.\n!pip install mne #The MNE Package is installed\nFILEID = \"1lo0MjWLvsyne2CgTA6VZ2HGY9SKxiwZ7\"\n!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id='$FILEID -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=\"$FILEID -O MI_EEG_ClassMeth.zip && rm -rf /tmp/cookies.txt\n!unzip MI_EEG_ClassMeth.zip #Package with useful functions for motor imagery classification based in EEG.\n!pip install -U git+https://github.com/UN-GCPDS/python-gcpds.EEG_Tensorflow_models.git\n!dir","metadata":{"id":"K0oS6IH7VTZX","execution":{"iopub.status.busy":"2024-12-06T20:56:30.280537Z","iopub.execute_input":"2024-12-06T20:56:30.281018Z","iopub.status.idle":"2024-12-06T20:57:32.369438Z","shell.execute_reply.started":"2024-12-06T20:56:30.280970Z","shell.execute_reply":"2024-12-06T20:57:32.368415Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!apt-get install --allow-change-held-packages libcudnn8=8.1.0.77-1+cuda11.2 -y\n!pip install tensorflow==2.8.2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T20:57:32.370883Z","iopub.execute_input":"2024-12-06T20:57:32.371173Z","iopub.status.idle":"2024-12-06T20:58:34.757505Z","shell.execute_reply.started":"2024-12-06T20:57:32.371145Z","shell.execute_reply":"2024-12-06T20:58:34.756360Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Functions","metadata":{}},{"cell_type":"code","source":"from gcpds.databases.BCI_Competition_IV import Dataset_2a\nfrom typing import Sequence, Tuple\nfrom MI_EEG_ClassMeth.FeatExtraction import TimeFrequencyRpr\nimport numpy as np\nfrom scipy.signal import resample\n\ndef load_BCICIV2a(db: Dataset_2a,\n               sbj: int,\n               mode: str,\n               fs: float, \n               f_bank: np.ndarray, \n               vwt: np.ndarray, \n               new_fs: float) -> np.ndarray:\n\n  tf_repr = TimeFrequencyRpr(sfreq = fs, f_bank = f_bank, vwt = vwt)\n\n  db.load_subject(sbj, mode = mode)\n    \n  X, y = db.get_data() #Load all classes, all channels {EEG, EOG}, reject bad trials\n\n  X = X[:,:-3,:] # pick EEG channels\n  X = X*1e6 #uV\n  X = np.squeeze(tf_repr.transform(X))\n  #Resampling\n  if new_fs == fs:\n    print('No resampling, since new sampling rate same.')\n  else:\n    print(\"Resampling from {:f} to {:f} Hz.\".format(fs, new_fs))\n    X = resample(X, int((X.shape[-1]/fs)*new_fs), axis = -1)\n    \n  return X, y\n\n\nfrom gcpds.databases import GIGA_MI_ME\n\ndef load_GIGA_MI_ME(db: GIGA_MI_ME,\n              sbj: int,\n              eeg_ch_names: Sequence[str],\n              fs: float, \n              f_bank: np.ndarray, \n              vwt: np.ndarray, \n              new_fs: float) -> Tuple[np.ndarray, np.ndarray]:\n\n    index_eeg_chs = db.format_channels_selectors(channels=eeg_ch_names) - 1\n\n    tf_repr = TimeFrequencyRpr(sfreq=fs, f_bank=f_bank, vwt=vwt)\n\n    # Load subject data\n    db.load_subject(sbj)\n    X, y = db.get_data(classes=['left hand mi', 'right hand mi'])\n    \n    # Debugging total trials\n    print(f\"Total trials loaded: {X.shape[0]}\")\n    print(f\"Shape of X: {X.shape}, Shape of y: {y.shape}\")\n\n    # Spatial rearrangement\n    X = X[:, index_eeg_chs, :]  \n    X = np.squeeze(tf_repr.transform(X))\n\n    # Resampling\n    if new_fs == fs:\n        print('No resampling, since new sampling rate is the same.')\n    else:\n        print(f\"Resampling from {fs} to {new_fs} Hz.\")\n        X = resample(X, int((X.shape[-1] / fs) * new_fs), axis=-1)\n    \n    return X, y\n\n\n\ndef load_DB(db_name, **load_args):\n  if db_name == 'BCICIV2a':\n    X_train, y_train = load_BCICIV2a(**load_args, mode = 'training')\n    X_test, y_test = load_BCICIV2a(**load_args, mode = 'evaluation')\n\n    X_train = np.concatenate([X_train, X_test], axis = 0)\n    y_train = np.concatenate([y_train, y_test], axis = 0)\n\n  elif db_name == 'GIGA_MI_ME':\n    X_train, y_train = load_GIGA_MI_ME(**load_args)\n    \n  else:\n    raise ValueError('No valid database name')\n\n  return X_train, y_train\n\n\nfrom EEG_Tensorflow_models.Models import DeepConvNet, ShallowConvNet, EEGNet, DMTL_BCI, TCNet_fusion, PST_attention\n\n\ndef get_model(model_name, nb_classes):\n  if model_name == 'DeepConvNet':\n    model = DeepConvNet\n    model_params = dict(nb_classes = nb_classes,\n                      dropoutRate = 0.5, version='2018')\n    \n  elif model_name == 'ShallowConvNet':\n    model = ShallowConvNet\n    model_params = dict(nb_classes = nb_classes,\n                      dropoutRate = 0.5,\n                      version = '2018')\n    \n  elif model_name == 'EEGNet':\n    model = EEGNet\n    model_params = dict(nb_classes = nb_classes,\n                      dropoutRate = 0.5,\n                      kernLength = 32,\n                      F1 = 8,\n                      D = 2,\n                      F2 = 16,\n                      norm_rate = 0.25, \n                      dropoutType = 'Dropout')\n    \n  elif model_name == 'DMTL_BCI':\n    model = DMTL_BCI\n    model_params = dict(nb_classes = nb_classes,\n                      dropoutRate = 0.5,\n                      l1 = 0,\n                      l2 = 0)\n    \n  elif model_name == 'TCNet_fusion':\n    model = TCNet_fusion\n    model_params = dict(nb_classes = nb_classes,\n                      layers = 2,\n                      kernel_s = 4,\n                      filt = 12,\n                      dropout = 0.3,\n                      activation = 'relu',\n                      F1 = 24,\n                      D = 2,\n                      kernLength = 32,\n                      N_residuals = 2)\n    \n  elif model_name == 'PST_attention':\n    model = PST_attention\n    model_params = dict(nb_classes = nb_classes,\n                      dropoutRate = 0.5,\n                      last_layer = 'Dense')\n    \n  else:\n    raise ValueError('No valid model name')\n    \n  return model, model_params\n\nfrom tensorflow.random import set_seed\nfrom tensorflow.keras.backend import clear_session\nfrom tensorflow.keras.optimizers import Adam\nfrom sklearn.metrics import accuracy_score, cohen_kappa_score, roc_auc_score,\\\n                            f1_score, recall_score, precision_score\n\ndef train(db_name, load_args, cv_args, model_args, compile_args, fit_args, seed):\n    X_train, y_train = load_DB(db_name, **load_args)\n    X_train = X_train[..., np.newaxis]\n    print(X_train.shape, 'tama√±o')\n    cv_results = {'params': [],\n                  'mean_acc': np.zeros(cv_args['cv'].get_n_splits()),\n                  'mean_kappa': np.zeros(cv_args['cv'].get_n_splits()),\n                  'mean_auc': np.zeros(cv_args['cv'].get_n_splits()),\n                  'mean_f1_left': np.zeros(cv_args['cv'].get_n_splits()),\n                  'mean_f1_right': np.zeros(cv_args['cv'].get_n_splits()),\n                  'mean_recall_left': np.zeros(cv_args['cv'].get_n_splits()),\n                  'mean_recall_right': np.zeros(cv_args['cv'].get_n_splits()),\n                  'mean_precision_left': np.zeros(cv_args['cv'].get_n_splits()),\n                  'mean_precision_right': np.zeros(cv_args['cv'].get_n_splits()),}\n    \n    if model_args['nb_classes'] == 4:\n        cv_results['mean_f1_legs'] = np.zeros(cv_args['cv'].get_n_splits())\n        cv_results['mean_f1_tongue'] = np.zeros(cv_args['cv'].get_n_splits())\n        cv_results['mean_recall_legs'] = np.zeros(cv_args['cv'].get_n_splits())\n        cv_results['mean_recall_tongue'] = np.zeros(cv_args['cv'].get_n_splits())\n        cv_results['mean_precision_legs'] = np.zeros(cv_args['cv'].get_n_splits())\n        cv_results['mean_precision_tongue'] = np.zeros(cv_args['cv'].get_n_splits())\n\n    k = 0\n    max_acc = -np.inf\n    for train_index, val_index in cv_args['cv'].split(X_train, y_train):\n      X, X_val = X_train[train_index], X_train[val_index]\n      y, y_val = y_train[train_index], y_train[val_index]\n      print(val_index)\n\n      print(X.shape,'valor total de X:')\n      print(X_val.shape, 'valor total de X_val:')\n        \n      if model_args['autoencoder']:\n        y = [X, y]\n\n      batch_size, C, T = X.shape[:-1]\n\n      clear_session()\n      set_seed(seed)\n\n      model_cll, model_params = get_model(model_args['model_name'], model_args['nb_classes'])\n      model = model_cll(**model_params, Chans = 64, Samples = T)\n      model.compile(loss = compile_args['loss'], \n                    optimizer = Adam(compile_args['init_lr']))\n        \n      print(X.shape)\n      print(y.shape)\n    \n      \n      history = model.fit(X, y,\n                batch_size = batch_size,\n                **fit_args)\n\n      if model_args['autoencoder']:\n        y_prob = model.predict(X_val)[-1]\n        y_pred = np.argmax(y_prob, axis = 1)\n      else:\n        y_prob = model.predict(X_val)\n        y_pred = np.argmax(y_prob, axis = 1)\n\n      cv_results['mean_acc'][k] = accuracy_score(y_val, y_pred)\n      cv_results['mean_kappa'][k] = cohen_kappa_score(y_val, y_pred)\n      if model_args['nb_classes'] == 2:\n        cv_results['mean_auc'][k] = roc_auc_score(y_val, y_prob[:, 1], average = 'macro')\n        cv_results['mean_f1_left'][k] = f1_score(y_val, y_pred, pos_label = 0, average = 'binary')\n        cv_results['mean_f1_right'][k] = f1_score(y_val, y_pred, pos_label = 1, average = 'binary')\n        cv_results['mean_recall_left'][k] = recall_score(y_val, y_pred, pos_label = 0, average = 'binary')\n        cv_results['mean_recall_right'][k] = recall_score(y_val, y_pred, pos_label = 1, average = 'binary')\n        cv_results['mean_precision_left'][k] = precision_score(y_val, y_pred, pos_label = 0, average = 'binary')\n        cv_results['mean_precision_right'][k] = precision_score(y_val, y_pred, pos_label = 1, average = 'binary')\n      else:                                                                                  \n        cv_results['mean_auc'][k] = roc_auc_score(y_val, y_prob, average = 'macro', multi_class = 'ovo')\n        \n        cv_results['mean_f1_left'][k] = f1_score(y_val, y_pred, pos_label = 0, average = 'micro')\n        cv_results['mean_f1_right'][k] = f1_score(y_val, y_pred, pos_label = 1, average = 'micro')\n        cv_results['mean_f1_legs'][k] = f1_score(y_val, y_pred, pos_label = 2, average = 'micro')\n        cv_results['mean_f1_tongue'][k] = f1_score(y_val, y_pred, pos_label = 3, average = 'micro')\n        cv_results['mean_recall_left'][k] = recall_score(y_val, y_pred, pos_label = 0, average = 'micro')\n        cv_results['mean_recall_right'][k] = recall_score(y_val, y_pred, pos_label = 1, average = 'micro')\n        cv_results['mean_recall_legs'][k] = recall_score(y_val, y_pred, pos_label = 2, average = 'micro')\n        cv_results['mean_recall_tongue'][k] = recall_score(y_val, y_pred, pos_label = 3, average = 'micro')\n        cv_results['mean_precision_left'][k] = precision_score(y_val, y_pred, pos_label = 0, average = 'micro')\n        cv_results['mean_precision_right'][k] = precision_score(y_val, y_pred, pos_label = 1, average = 'micro')\n        cv_results['mean_precision_legs'][k] = precision_score(y_val, y_pred, pos_label = 2, average = 'micro')\n        cv_results['mean_precision_tongue'][k] = precision_score(y_val, y_pred, pos_label = 3, average = 'micro')\n                                                       \n                                                       \n    if cv_results['mean_acc'][k] > max_acc:\n        print('New Max Found!')\n        max_acc = cv_results['mean_acc'][k]\n    \n        # Guarda los pesos del mejor modelo\n        model.save_weights(f'sbj{load_args[\"sbj\"]}.h5')\n    \n        # Actualiza los datos del mejor fold\n        cv_results['best_fold'] = {\n            'fold_index': k,  # N√∫mero del fold\n            'train_indices': train_index.tolist(),  # √çndices de entrenamiento\n            'val_indices': val_index.tolist(),  # √çndices de validaci√≥n\n            'accuracy': cv_results['mean_acc'][k],\n            'kappa': cv_results['mean_kappa'][k],\n            'auc': cv_results['mean_auc'][k],\n            'f1_left': cv_results['mean_f1_left'][k],\n            'f1_right': cv_results['mean_f1_right'][k],\n            'recall_left': cv_results['mean_recall_left'][k],\n            'recall_right': cv_results['mean_recall_right'][k],\n            'precision_left': cv_results['mean_precision_left'][k],\n            'precision_right': cv_results['mean_precision_right'][k],\n        }\n\n    k += 1\n                                                \n    cv_results['std_acc'] = round(cv_results['mean_acc'].std(), 3)\n    cv_results['mean_acc'] = round(cv_results['mean_acc'].mean(), 3)\n    cv_results['std_kappa'] = round(cv_results['mean_kappa'].std(), 3)\n    cv_results['mean_kappa'] = round(cv_results['mean_kappa'].mean(), 3)\n    cv_results['std_auc'] = round(cv_results['mean_auc'].std(), 3)\n    cv_results['mean_auc'] = round(cv_results['mean_auc'].mean(), 3)\n      \n    cv_results['mean_f1_left'] = round(cv_results['mean_f1_left'].mean(), 3)\n    cv_results['std_f1_left'] = round(cv_results['mean_f1_left'].std(), 3)\n    cv_results['mean_f1_right'] = round(cv_results['mean_f1_right'].mean(), 3)\n    cv_results['std_f1_right'] = round(cv_results['mean_f1_right'].std(), 3)\n    cv_results['mean_recall_left'] = round(cv_results['mean_recall_left'].mean(), 3)\n    cv_results['std_recall_left'] = round(cv_results['mean_recall_left'].std(), 3)\n    cv_results['mean_recall_right'] = round(cv_results['mean_recall_right'].mean(), 3)\n    cv_results['std_recall_right'] = round(cv_results['mean_recall_right'].std(), 3)\n    cv_results['mean_precision_left'] = round(cv_results['mean_precision_left'].mean(), 3)\n    cv_results['std_precision_left'] = round(cv_results['mean_precision_left'].std(), 3)\n    cv_results['mean_precision_right'] = round(cv_results['mean_precision_right'].mean(), 3)\n    cv_results['std_precision_right'] = round(cv_results['mean_precision_right'].std(), 3)\n\n    if model_args['nb_classes'] == 4:\n        cv_results['mean_f1_legs'] = round(cv_results['mean_f1_legs'].mean(), 3)\n        cv_results['std_f1_legs'] = round(cv_results['mean_f1_legs'].std(), 3)\n        cv_results['mean_f1_tongue'] = round(cv_results['mean_f1_tongue'].mean(), 3)\n        cv_results['std_f1_tongue'] = round(cv_results['mean_f1_tongue'].std(), 3)\n        cv_results['mean_recall_legs'] = round(cv_results['mean_recall_legs'].mean(), 3)\n        cv_results['std_recall_legs'] = round(cv_results['mean_recall_legs'].std(), 3)\n        cv_results['mean_recall_tongue'] = round(cv_results['mean_recall_tongue'].mean(), 3)\n        cv_results['std_recall_tongue'] = round(cv_results['mean_recall_tongue'].std(), 3)\n        cv_results['mean_precision_legs'] = round(cv_results['mean_precision_legs'].mean(), 3)\n        cv_results['std_precision_legs'] = round(cv_results['mean_precision_legs'].std(), 3)\n        cv_results['mean_precision_tongue'] = round(cv_results['mean_precision_tongue'].mean(), 3)\n        cv_results['std_precision_tongue'] = round(cv_results['mean_precision_tongue'].std(), 3)\n    \n    return cv_results","metadata":{"id":"yE1sbHYQVbBq","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Config","metadata":{"id":"uBAeW6J5S68g"}},{"cell_type":"code","source":"# Marcos, use these two variables to run the state of the art. First, for BCICIV2a run all the models.\n# Remeber that this network DMTL_BCI is an autoencoder. Set the nb_classses parameter depending of the database.\n# set autoencoder based on the model\n# We need to run all these tests again. Do not forget to add the recall, preci, and f1 for each class (bci 4, giga 2)\ndb_name = 'GIGA_MI_ME'\nmodel_args = dict(model_name = 'EEGNet',\n                  nb_classes = 2,\n                  autoencoder = False)","metadata":{"id":"2I3IQnNSS9-a","trusted":true,"execution":{"iopub.status.busy":"2024-12-06T20:58:40.890936Z","iopub.execute_input":"2024-12-06T20:58:40.891488Z","iopub.status.idle":"2024-12-06T20:58:40.895861Z","shell.execute_reply.started":"2024-12-06T20:58:40.891440Z","shell.execute_reply":"2024-12-06T20:58:40.894913Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.callbacks import ReduceLROnPlateau, TerminateOnNaN\nimport numpy as np\nfrom tensorflow.keras.losses import SparseCategoricalCrossentropy, MeanSquaredError\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\nif db_name == 'BCICIV2a':\n  db = Dataset_2a('/kaggle/input/dataset-2a')\n  fs = db.metadata['sampling_rate']\n  load_args = dict(db = db,\n                 fs = fs,\n                 f_bank = np.asarray([[4., 40.]]),\n                 vwt = np.asarray([[2.5, 6]]),\n                 new_fs = 128.)\n  subjects = np.arange(db.metadata['subjects']) + 1\n  \nelif db_name == 'GIGA_MI_ME':\n  db = GIGA_MI_ME('/kaggle/input/giga-science-gcpds/GIGA_MI_ME')\n  fs = db.metadata['sampling_rate']\n  \n    \n  eeg_ch_names = ['Fp1','Fpz','Fp2',\n                 'AF7','AF3','AFz','AF4','AF8',\n                'F7','F5','F3','F1','Fz','F2','F4','F6','F8',\n               'FT7','FC5','FC3','FC1','FCz','FC2','FC4','FC6','FT8',\n                'T7','C5','C3','C1','Cz','C2','C4','C6','T8',\n               'TP7','CP5','CP3','CP1','CPz','CP2','CP4','CP6','TP8',\n                'P9','P7','P5','P3','P1','Pz','P2','P4','P6','P8','P10',\n                'PO7','PO3','POz','PO4','PO8',\n                'O1','Oz','O2',\n                'Iz']\n\n # eeg_ch_names = ['Fp1','Fp2',\n #                'T7','C3','C4','T8',\n #                'O1','O2']\n\n # eeg_ch_names = ['Fp1','Fp2',\n #               'F7','F3','F4','F8',\n #               'T7','C3','C4','T8',\n #               'P7','P3','P4','P8',\n #               'O1','O2']\n\n # eeg_ch_names = ['Fp1','Fp2',\n #                'AF3','AF4',\n #                'F7','F3','Fz','F4','F8',\n #                'FC5','FC1','FC2','FC6',\n #                'T7','C3','Cz','C4','T8',\n #                'CP5','CP1','CP2','CP6',\n #                'P7','P3','Pz','P4','P8',\n #                'PO3','PO4',\n #                'O1','Oz','O2']\n\n\n  load_args = dict(db = db,\n                  eeg_ch_names = eeg_ch_names,\n                  fs = fs,\n                  f_bank = np.asarray([[4., 40.]]),\n                  vwt = np.asarray([[2.5, 5]]),\n                  new_fs = 128.)\n  subjects = np.arange(db.metadata['subjects']) + 1\n  subjects = np.delete(subjects, [28,33])\n  \nelse:\n  raise ValueError('No valid database name')\n\nverbose = 0\nreduce_lr_on_plateau = ReduceLROnPlateau(monitor = 'loss', factor = 0.1, patience = 30, verbose = verbose, mode = 'min', min_delta = 0.01, min_lr = 0)\nterminate_on_nan = TerminateOnNaN()\ncallbacks = [reduce_lr_on_plateau, terminate_on_nan]\nseed = 23\n\ncv_args = dict(cv = StratifiedShuffleSplit(n_splits = 5, test_size = 0.2, random_state = seed))\n\ncompile_args = dict(loss = SparseCategoricalCrossentropy(), #['mse' , SparseCategoricalCrossentropy()]\n                    init_lr = 1e-2)\n                      \nfit_args = dict(epochs = 500,\n                verbose = verbose,\n                callbacks = callbacks)","metadata":{"id":"tqMhUFoBIc3B","outputId":"1405fd59-1374-4d5d-8e3a-5e7a45c79bba","trusted":true,"execution":{"iopub.status.busy":"2024-12-06T20:58:40.897411Z","iopub.execute_input":"2024-12-06T20:58:40.897852Z","iopub.status.idle":"2024-12-06T20:58:40.917037Z","shell.execute_reply.started":"2024-12-06T20:58:40.897800Z","shell.execute_reply":"2024-12-06T20:58:40.916330Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Main","metadata":{"id":"ukhXifxzTaj9"}},{"cell_type":"code","source":"from pickle import dump\n\nsubjects = [10]\n\nfor sbj in subjects[:]:\n  print('sbj = ', sbj)\n  load_args['sbj'] = sbj\n  results = train(db_name, load_args, cv_args, model_args, compile_args, fit_args, seed)\n  with open('sbj' + str(load_args['sbj']) + '.txt', 'wb') as f:\n    dump(results, f)","metadata":{"id":"Ymqd_W21y3NK","outputId":"5ca97a2f-f57c-46ee-8f53-f00181ccea90","scrolled":true,"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T20:58:40.918041Z","iopub.execute_input":"2024-12-06T20:58:40.918300Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!zip sbj3_5_64ch_EEGNet.zip ./*.h5\n!zip sbj3_5_64ch_EEGNet.zip ./*.txt","metadata":{"id":"V7-P0xjwzXVX","outputId":"270dceef-351d-48d1-f71e-2c3367c7fdac","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#import pickle as pkl\n\n#with open(file= '/kaggle/working/sbj14.txt', mode = 'rb' ) as f:\n#    results_64ch_ShallowConvNet = pkl.load(f)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#results_64ch_ShallowConvNet","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#with open(file= '/kaggle/working/sbj2.txt', mode = 'rb' ) as f:\n #   results_64ch_ShallowConvNet = pkl.load(f)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#results_64ch_ShallowConvNet","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}