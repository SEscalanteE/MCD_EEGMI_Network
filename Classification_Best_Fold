{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfd140ab",
   "metadata": {
    "id": "x9LNzEYERaH2",
    "papermill": {
     "duration": 0.004596,
     "end_time": "2025-08-22T16:32:13.799319",
     "exception": false,
     "start_time": "2025-08-22T16:32:13.794723",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Download Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af776ba4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T16:32:13.809355Z",
     "iopub.status.busy": "2025-08-22T16:32:13.808838Z",
     "iopub.status.idle": "2025-08-22T16:32:14.863757Z",
     "shell.execute_reply": "2025-08-22T16:32:14.862941Z"
    },
    "papermill": {
     "duration": 1.062112,
     "end_time": "2025-08-22T16:32:14.865744",
     "exception": false,
     "start_time": "2025-08-22T16:32:13.803632",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: Tesla T4 (UUID: GPU-ec6cbb97-86b0-8cc9-4168-d155e2d2deeb)\r\n",
      "GPU 1: Tesla T4 (UUID: GPU-3c03e977-0de9-c979-386a-d7a47b86674a)\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5662e304",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T16:32:14.873365Z",
     "iopub.status.busy": "2025-08-22T16:32:14.873065Z",
     "iopub.status.idle": "2025-08-22T16:33:01.879060Z",
     "shell.execute_reply": "2025-08-22T16:33:01.877878Z"
    },
    "id": "K0oS6IH7VTZX",
    "papermill": {
     "duration": 47.01204,
     "end_time": "2025-08-22T16:33:01.881154",
     "exception": false,
     "start_time": "2025-08-22T16:32:14.869114",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -U git+https://github.com/UN-GCPDS/python-gcpds.databases #Package for database reading.\n",
    "!pip install mne #The MNE Package is installed\n",
    "FILEID = \"1lo0MjWLvsyne2CgTA6VZ2HGY9SKxiwZ7\"\n",
    "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id='$FILEID -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=\"$FILEID -O MI_EEG_ClassMeth.zip && rm -rf /tmp/cookies.txt\n",
    "!unzip MI_EEG_ClassMeth.zip #Package with useful functions for motor imagery classification based in EEG.\n",
    "!pip install -U git+https://github.com/UN-GCPDS/python-gcpds.EEG_Tensorflow_models.git\n",
    "!dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcf5a981",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T16:33:01.889189Z",
     "iopub.status.busy": "2025-08-22T16:33:01.888851Z",
     "iopub.status.idle": "2025-08-22T16:34:29.741041Z",
     "shell.execute_reply": "2025-08-22T16:34:29.739895Z"
    },
    "papermill": {
     "duration": 87.859933,
     "end_time": "2025-08-22T16:34:29.744494",
     "exception": false,
     "start_time": "2025-08-22T16:33:01.884561",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\r\n",
      "\r\n",
      "Package libcudnn8 is not available, but is referred to by another package.\r\n",
      "This may mean that the package is missing, has been obsoleted, or\r\n",
      "is only available from another source\r\n",
      "\r\n",
      "E: Version '8.1.0.77-1+cuda11.2' for 'libcudnn8' was not found\r\n",
      "Collecting tensorflow==2.8.2\r\n",
      "  Downloading tensorflow-2.8.2-cp310-cp310-manylinux2010_x86_64.whl.metadata (2.9 kB)\r\n",
      "Requirement already satisfied: absl-py>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.8.2) (1.4.0)\r\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.8.2) (1.6.3)\r\n",
      "Requirement already satisfied: flatbuffers>=1.12 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.8.2) (24.3.25)\r\n",
      "Requirement already satisfied: gast>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.8.2) (0.5.4)\r\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.8.2) (0.2.0)\r\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.8.2) (3.11.0)\r\n",
      "Collecting keras-preprocessing>=1.1.1 (from tensorflow==2.8.2)\r\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl.metadata (1.9 kB)\r\n",
      "Requirement already satisfied: libclang>=9.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.8.2) (18.1.1)\r\n",
      "Requirement already satisfied: numpy>=1.20 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.8.2) (1.26.4)\r\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.8.2) (3.3.0)\r\n",
      "Collecting protobuf<3.20,>=3.9.2 (from tensorflow==2.8.2)\r\n",
      "  Downloading protobuf-3.19.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (787 bytes)\r\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.8.2) (70.0.0)\r\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.8.2) (1.16.0)\r\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.8.2) (2.4.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.8.2) (4.12.2)\r\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.8.2) (1.16.0)\r\n",
      "Collecting tensorboard<2.9,>=2.8 (from tensorflow==2.8.2)\r\n",
      "  Downloading tensorboard-2.8.0-py3-none-any.whl.metadata (1.9 kB)\r\n",
      "Collecting tensorflow-estimator<2.9,>=2.8 (from tensorflow==2.8.2)\r\n",
      "  Downloading tensorflow_estimator-2.8.0-py2.py3-none-any.whl.metadata (1.3 kB)\r\n",
      "Collecting keras<2.9,>=2.8.0rc0 (from tensorflow==2.8.2)\r\n",
      "  Downloading keras-2.8.0-py2.py3-none-any.whl.metadata (1.3 kB)\r\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.8.2) (0.37.0)\r\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.8.2) (1.62.2)\r\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow==2.8.2) (0.43.0)\r\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.2) (2.30.0)\r\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.9,>=2.8->tensorflow==2.8.2)\r\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl.metadata (2.7 kB)\r\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.2) (3.6)\r\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.2) (2.32.3)\r\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard<2.9,>=2.8->tensorflow==2.8.2)\r\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl.metadata (1.1 kB)\r\n",
      "Collecting tensorboard-plugin-wit>=1.6.0 (from tensorboard<2.9,>=2.8->tensorflow==2.8.2)\r\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl.metadata (873 bytes)\r\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.2) (3.0.4)\r\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.2) (4.2.4)\r\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.2) (0.4.0)\r\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.2) (4.9)\r\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8.2) (2.0.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.2) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.2) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.2) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.2) (2024.8.30)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=0.11.15->tensorboard<2.9,>=2.8->tensorflow==2.8.2) (2.1.5)\r\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.2) (0.6.0)\r\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8.2) (3.2.2)\r\n",
      "Downloading tensorflow-2.8.2-cp310-cp310-manylinux2010_x86_64.whl (498.0 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m498.0/498.0 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading keras-2.8.0-py2.py3-none-any.whl (1.4 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading protobuf-3.19.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m95.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading tensorflow_estimator-2.8.0-py2.py3-none-any.whl (462 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.3/462.3 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\r\n",
      "Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m78.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: tensorflow-estimator, tensorboard-plugin-wit, keras, tensorboard-data-server, protobuf, keras-preprocessing, google-auth-oauthlib, tensorboard, tensorflow\r\n",
      "  Attempting uninstall: tensorflow-estimator\r\n",
      "    Found existing installation: tensorflow-estimator 2.15.0\r\n",
      "    Uninstalling tensorflow-estimator-2.15.0:\r\n",
      "      Successfully uninstalled tensorflow-estimator-2.15.0\r\n",
      "  Attempting uninstall: keras\r\n",
      "    Found existing installation: keras 3.3.3\r\n",
      "    Uninstalling keras-3.3.3:\r\n",
      "      Successfully uninstalled keras-3.3.3\r\n",
      "  Attempting uninstall: tensorboard-data-server\r\n",
      "    Found existing installation: tensorboard-data-server 0.7.2\r\n",
      "    Uninstalling tensorboard-data-server-0.7.2:\r\n",
      "      Successfully uninstalled tensorboard-data-server-0.7.2\r\n",
      "  Attempting uninstall: protobuf\r\n",
      "    Found existing installation: protobuf 3.20.3\r\n",
      "    Uninstalling protobuf-3.20.3:\r\n",
      "      Successfully uninstalled protobuf-3.20.3\r\n",
      "  Attempting uninstall: google-auth-oauthlib\r\n",
      "    Found existing installation: google-auth-oauthlib 1.2.0\r\n",
      "    Uninstalling google-auth-oauthlib-1.2.0:\r\n",
      "      Successfully uninstalled google-auth-oauthlib-1.2.0\r\n",
      "  Attempting uninstall: tensorboard\r\n",
      "    Found existing installation: tensorboard 2.16.2\r\n",
      "    Uninstalling tensorboard-2.16.2:\r\n",
      "      Successfully uninstalled tensorboard-2.16.2\r\n",
      "  Attempting uninstall: tensorflow\r\n",
      "    Found existing installation: tensorflow 2.16.1\r\n",
      "    Uninstalling tensorflow-2.16.1:\r\n",
      "      Successfully uninstalled tensorflow-2.16.1\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "apache-beam 2.46.0 requires cloudpickle~=2.2.1, but you have cloudpickle 3.0.0 which is incompatible.\r\n",
      "apache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\r\n",
      "apache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\r\n",
      "apache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 16.1.0 which is incompatible.\r\n",
      "google-ai-generativelanguage 0.6.10 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\r\n",
      "google-cloud-aiplatform 0.6.0a1 requires google-api-core[grpc]<2.0.0dev,>=1.22.2, but you have google-api-core 2.11.1 which is incompatible.\r\n",
      "google-cloud-automl 1.0.1 requires google-api-core[grpc]<2.0.0dev,>=1.14.0, but you have google-api-core 2.11.1 which is incompatible.\r\n",
      "google-cloud-datastore 2.20.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\r\n",
      "google-cloud-language 2.14.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\r\n",
      "google-cloud-spanner 3.47.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\r\n",
      "google-cloud-videointelligence 2.13.5 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\r\n",
      "kfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\r\n",
      "onnx 1.17.0 requires protobuf>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\r\n",
      "tensorboardx 2.6.2.2 requires protobuf>=3.20, but you have protobuf 3.19.6 which is incompatible.\r\n",
      "tensorflow-datasets 4.9.6 requires protobuf>=3.20, but you have protobuf 3.19.6 which is incompatible.\r\n",
      "tensorflow-decision-forests 1.9.1 requires tensorflow~=2.16.1, but you have tensorflow 2.8.2 which is incompatible.\r\n",
      "tensorflow-serving-api 2.16.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 3.19.6 which is incompatible.\r\n",
      "tensorflow-serving-api 2.16.1 requires tensorflow<3,>=2.16.1, but you have tensorflow 2.8.2 which is incompatible.\r\n",
      "tensorflow-text 2.16.1 requires tensorflow<2.17,>=2.16.1; platform_machine != \"arm64\" or platform_system != \"Darwin\", but you have tensorflow 2.8.2 which is incompatible.\r\n",
      "tf-keras 2.16.0 requires tensorflow<2.17,>=2.16, but you have tensorflow 2.8.2 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed google-auth-oauthlib-0.4.6 keras-2.8.0 keras-preprocessing-1.1.2 protobuf-3.19.6 tensorboard-2.8.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.8.2 tensorflow-estimator-2.8.0\r\n"
     ]
    }
   ],
   "source": [
    "!apt-get install --allow-change-held-packages libcudnn8=8.1.0.77-1+cuda11.2 -y\n",
    "!pip install tensorflow==2.8.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab4d3b1",
   "metadata": {
    "papermill": {
     "duration": 0.011814,
     "end_time": "2025-08-22T16:34:29.770614",
     "exception": false,
     "start_time": "2025-08-22T16:34:29.758800",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "802a37a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T16:34:29.809380Z",
     "iopub.status.busy": "2025-08-22T16:34:29.808877Z",
     "iopub.status.idle": "2025-08-22T16:34:41.554711Z",
     "shell.execute_reply": "2025-08-22T16:34:41.554022Z"
    },
    "id": "yE1sbHYQVbBq",
    "papermill": {
     "duration": 11.773457,
     "end_time": "2025-08-22T16:34:41.556737",
     "exception": false,
     "start_time": "2025-08-22T16:34:29.783280",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gcpds.databases.BCI_Competition_IV import Dataset_2a\n",
    "from typing import Sequence, Tuple\n",
    "from MI_EEG_ClassMeth.FeatExtraction import TimeFrequencyRpr\n",
    "import numpy as np\n",
    "from scipy.signal import resample\n",
    "\n",
    "def load_BCICIV2a(db: Dataset_2a,\n",
    "               sbj: int,\n",
    "               mode: str,\n",
    "               fs: float, \n",
    "               f_bank: np.ndarray, \n",
    "               vwt: np.ndarray, \n",
    "               new_fs: float) -> np.ndarray:\n",
    "\n",
    "  tf_repr = TimeFrequencyRpr(sfreq = fs, f_bank = f_bank, vwt = vwt)\n",
    "\n",
    "  db.load_subject(sbj, mode = mode)\n",
    "\n",
    "    \n",
    "  X_all, y_all = db.get_data()  #Load all classes, all channels {EEG, EOG}, reject bad trials\n",
    "\n",
    "  indices_to_keep = np.where((y_all == 0) | (y_all == 1))[0]\n",
    "  X = X_all[indices_to_keep]\n",
    "  y = y_all[indices_to_keep]\n",
    "  print(f\"Filtered from {len(y_all)} trials to {len(y)} trials (classes 0 and 1 only).\")\n",
    "\n",
    "    \n",
    "  X = X[:,:-3,:] # pick EEG channels\n",
    "  X = X*1e6 #uV\n",
    "  X = np.squeeze(tf_repr.transform(X))\n",
    "  #Resampling\n",
    "  if new_fs == fs:\n",
    "    print('No resampling, since new sampling rate same.')\n",
    "  else:\n",
    "    print(\"Resampling from {:f} to {:f} Hz.\".format(fs, new_fs))\n",
    "    X = resample(X, int((X.shape[-1]/fs)*new_fs), axis = -1)\n",
    "    \n",
    "  return X, y\n",
    "\n",
    "\n",
    "from gcpds.databases import GIGA_MI_ME\n",
    "\n",
    "def load_GIGA_MI_ME(db: GIGA_MI_ME,\n",
    "              sbj: int,\n",
    "              eeg_ch_names: Sequence[str],\n",
    "              fs: float, \n",
    "              f_bank: np.ndarray, \n",
    "              vwt: np.ndarray, \n",
    "              new_fs: float) -> Tuple[np.ndarray, np.ndarray]:\n",
    "\n",
    "    index_eeg_chs = db.format_channels_selectors(channels=eeg_ch_names) - 1\n",
    "\n",
    "    tf_repr = TimeFrequencyRpr(sfreq=fs, f_bank=f_bank, vwt=vwt)\n",
    "\n",
    "    # Load subject data\n",
    "    db.load_subject(sbj)\n",
    "    X, y = db.get_data(classes=['left hand mi', 'right hand mi'])\n",
    "    \n",
    "    # Debugging total trials\n",
    "    print(f\"Total trials loaded: {X.shape[0]}\")\n",
    "    print(f\"Shape of X: {X.shape}, Shape of y: {y.shape}\")\n",
    "\n",
    "    # Spatial rearrangement\n",
    "    X = X[:, index_eeg_chs, :]  \n",
    "    X = np.squeeze(tf_repr.transform(X))\n",
    "\n",
    "    # Resampling\n",
    "    if new_fs == fs:\n",
    "        print('No resampling, since new sampling rate is the same.')\n",
    "    else:\n",
    "        print(f\"Resampling from {fs} to {new_fs} Hz.\")\n",
    "        X = resample(X, int((X.shape[-1] / fs) * new_fs), axis=-1)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "\n",
    "\n",
    "def load_DB(db_name, **load_args):\n",
    "  if db_name == 'BCICIV2a':\n",
    "    X_train, y_train = load_BCICIV2a(**load_args, mode = 'training')\n",
    "    X_test, y_test = load_BCICIV2a(**load_args, mode = 'evaluation')\n",
    "\n",
    "    X_train = np.concatenate([X_train, X_test], axis = 0)\n",
    "    y_train = np.concatenate([y_train, y_test], axis = 0)\n",
    "\n",
    "  elif db_name == 'GIGA_MI_ME':\n",
    "    X_train, y_train = load_GIGA_MI_ME(**load_args)\n",
    "    \n",
    "  else:\n",
    "    raise ValueError('No valid database name')\n",
    "\n",
    "  return X_train, y_train\n",
    "\n",
    "\n",
    "from EEG_Tensorflow_models.Models import DeepConvNet, ShallowConvNet, EEGNet, DMTL_BCI, TCNet_fusion, PST_attention\n",
    "\n",
    "\n",
    "def get_model(model_name, nb_classes):\n",
    "  if model_name == 'DeepConvNet':\n",
    "    model = DeepConvNet\n",
    "    model_params = dict(nb_classes = nb_classes,\n",
    "                      dropoutRate = 0.5, version='2018')\n",
    "    \n",
    "  elif model_name == 'ShallowConvNet':\n",
    "    model = ShallowConvNet\n",
    "    model_params = dict(nb_classes = nb_classes,\n",
    "                      dropoutRate = 0.5,\n",
    "                      version = '2018')\n",
    "    \n",
    "  elif model_name == 'EEGNet':\n",
    "    model = EEGNet\n",
    "    model_params = dict(nb_classes = nb_classes,\n",
    "                      dropoutRate = 0.5,\n",
    "                      kernLength = 32,\n",
    "                      F1 = 8,\n",
    "                      D = 2,\n",
    "                      F2 = 16,\n",
    "                      norm_rate = 0.25, \n",
    "                      dropoutType = 'Dropout')\n",
    "    \n",
    "  elif model_name == 'DMTL_BCI':\n",
    "    model = DMTL_BCI\n",
    "    model_params = dict(nb_classes = nb_classes,\n",
    "                      dropoutRate = 0.5,\n",
    "                      l1 = 0,\n",
    "                      l2 = 0)\n",
    "    \n",
    "  elif model_name == 'TCNet_fusion':\n",
    "    model = TCNet_fusion\n",
    "    model_params = dict(nb_classes = nb_classes,\n",
    "                      layers = 2,\n",
    "                      kernel_s = 4,\n",
    "                      filt = 12,\n",
    "                      dropout = 0.3,\n",
    "                      activation = 'relu',\n",
    "                      F1 = 24,\n",
    "                      D = 2,\n",
    "                      kernLength = 32,\n",
    "                      N_residuals = 2)\n",
    "    \n",
    "  elif model_name == 'PST_attention':\n",
    "    model = PST_attention\n",
    "    model_params = dict(nb_classes = nb_classes,\n",
    "                      dropoutRate = 0.5,\n",
    "                      last_layer = 'Dense')\n",
    "    \n",
    "  else:\n",
    "    raise ValueError('No valid model name')\n",
    "    \n",
    "  return model, model_params\n",
    "\n",
    "from tensorflow.random import set_seed\n",
    "from tensorflow.keras.backend import clear_session\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import accuracy_score, cohen_kappa_score, roc_auc_score,\\\n",
    "                            f1_score, recall_score, precision_score\n",
    "\n",
    "def train(db_name, load_args, cv_args, model_args, compile_args, fit_args, seed):\n",
    "    X_train, y_train = load_DB(db_name, **load_args)\n",
    "    X_train = X_train[..., np.newaxis]  # Add channel dimension\n",
    "    print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "    \n",
    "    cv_results = {'params': [],\n",
    "                  'mean_acc': np.zeros(cv_args['cv'].get_n_splits()),\n",
    "                  'mean_kappa': np.zeros(cv_args['cv'].get_n_splits()),\n",
    "                  'mean_auc': np.zeros(cv_args['cv'].get_n_splits()),\n",
    "                  'mean_f1_left': np.zeros(cv_args['cv'].get_n_splits()),\n",
    "                  'mean_f1_right': np.zeros(cv_args['cv'].get_n_splits()),\n",
    "                  'mean_recall_left': np.zeros(cv_args['cv'].get_n_splits()),\n",
    "                  'mean_recall_right': np.zeros(cv_args['cv'].get_n_splits()),\n",
    "                  'mean_precision_left': np.zeros(cv_args['cv'].get_n_splits()),\n",
    "                  'mean_precision_right': np.zeros(cv_args['cv'].get_n_splits()),\n",
    "                  'all_folds': []}\n",
    "    \n",
    "    k = 0\n",
    "    max_acc = -np.inf\n",
    "\n",
    "    # Loop through folds\n",
    "    for train_index, val_index in cv_args['cv'].split(X_train, y_train):\n",
    "        print(f\"Running fold {k} with {len(train_index)} training samples and {len(val_index)} validation samples\")\n",
    "        print(f\"Fold {k}: train indices: {train_index[:5]}, val indices: {val_index[:5]}\")  # Print first indices\n",
    "        \n",
    "        X, X_val = X_train[train_index], X_train[val_index]\n",
    "        y, y_val = y_train[train_index], y_train[val_index]\n",
    "        \n",
    "        if model_args['autoencoder']:\n",
    "            y = [X, y]\n",
    "        \n",
    "        print(f\"Training data shape: {X.shape}, Validation data shape: {X_val.shape}\")\n",
    "        \n",
    "        batch_size, C, T = X.shape[:-1]\n",
    "        clear_session()\n",
    "        set_seed(seed)\n",
    "\n",
    "        model_cll, model_params = get_model(model_args['model_name'], model_args['nb_classes'])\n",
    "        model = model_cll(**model_params, Chans=22, Samples=T)\n",
    "        model.compile(loss=compile_args['loss'], optimizer=Adam(compile_args['init_lr']))\n",
    "\n",
    "        history = model.fit(X, y, batch_size=batch_size, **fit_args)\n",
    "        print(f\"Fold {k} training loss: {history.history['loss'][-1]}\")  # Print final loss\n",
    "\n",
    "        if model_args['autoencoder']:\n",
    "            y_prob = model.predict(X_val)[-1]\n",
    "        else:\n",
    "            y_prob = model.predict(X_val)\n",
    "        y_pred = np.argmax(y_prob, axis=1)\n",
    "\n",
    "        print(f\"y_true (val): {y_val[:5]}\")\n",
    "        print(f\"y_pred: {y_pred[:5]}\")\n",
    "        \n",
    "        # Calculate metrics\n",
    "        acc = accuracy_score(y_val, y_pred)\n",
    "        kappa = cohen_kappa_score(y_val, y_pred)\n",
    "        auc = roc_auc_score(y_val, y_prob[:, 1], average='macro') if model_args['nb_classes'] == 2 else None\n",
    "        \n",
    "        # Save metrics for this fold\n",
    "        fold_result = {\n",
    "            'fold_index': k,\n",
    "            'train_indices': train_index.tolist(),\n",
    "            'val_indices': val_index.tolist(),\n",
    "            'accuracy': acc,\n",
    "            'kappa': kappa,\n",
    "            'auc': auc\n",
    "        }\n",
    "        print(f\"Appending results for fold {k}: {fold_result}\")\n",
    "        cv_results['all_folds'].append(fold_result)\n",
    "\n",
    "        # Update overall fold metrics\n",
    "        cv_results['mean_acc'][k] = acc\n",
    "        cv_results['mean_kappa'][k] = kappa\n",
    "        if auc is not None:\n",
    "            cv_results['mean_auc'][k] = auc\n",
    "        \n",
    "        # Save the best model weights\n",
    "        if acc > max_acc:\n",
    "            print('New Max Found!')\n",
    "            max_acc = acc\n",
    "            model.save_weights(f'sbj{load_args[\"sbj\"]}.h5')\n",
    "\n",
    "        k += 1\n",
    "    \n",
    "    # Calculate mean and std metrics\n",
    "    cv_results['std_acc'] = round(cv_results['mean_acc'].std(), 3)\n",
    "    cv_results['mean_acc'] = round(cv_results['mean_acc'].mean(), 3)\n",
    "    cv_results['std_kappa'] = round(cv_results['mean_kappa'].std(), 3)\n",
    "    cv_results['mean_kappa'] = round(cv_results['mean_kappa'].mean(), 3)\n",
    "    cv_results['std_auc'] = round(cv_results['mean_auc'].std(), 3)\n",
    "    cv_results['mean_auc'] = round(cv_results['mean_auc'].mean(), 3)\n",
    "    \n",
    "    print(f\"Final cross-validation results: {cv_results}\")\n",
    "    return cv_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee7a8d7",
   "metadata": {
    "id": "uBAeW6J5S68g",
    "papermill": {
     "duration": 0.012262,
     "end_time": "2025-08-22T16:34:41.581336",
     "exception": false,
     "start_time": "2025-08-22T16:34:41.569074",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74a7336b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T16:34:41.605935Z",
     "iopub.status.busy": "2025-08-22T16:34:41.605394Z",
     "iopub.status.idle": "2025-08-22T16:34:41.609683Z",
     "shell.execute_reply": "2025-08-22T16:34:41.608875Z"
    },
    "id": "2I3IQnNSS9-a",
    "papermill": {
     "duration": 0.018305,
     "end_time": "2025-08-22T16:34:41.611305",
     "exception": false,
     "start_time": "2025-08-22T16:34:41.593000",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Marcos, use these two variables to run the state of the art. First, for BCICIV2a run all the models.\n",
    "# Remeber that this network DMTL_BCI is an autoencoder. Set the nb_classses parameter depending of the database.\n",
    "# set autoencoder based on the model\n",
    "# We need to run all these tests again. Do not forget to add the recall, preci, and f1 for each class (bci 4, giga 2)\n",
    "db_name = 'BCICIV2a'\n",
    "model_args = dict(model_name = 'TCNet_fusion',\n",
    "                  nb_classes = 2,\n",
    "                  autoencoder = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da3d2d32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T16:34:41.636196Z",
     "iopub.status.busy": "2025-08-22T16:34:41.635937Z",
     "iopub.status.idle": "2025-08-22T16:34:41.647302Z",
     "shell.execute_reply": "2025-08-22T16:34:41.646505Z"
    },
    "id": "tqMhUFoBIc3B",
    "outputId": "1405fd59-1374-4d5d-8e3a-5e7a45c79bba",
    "papermill": {
     "duration": 0.025802,
     "end_time": "2025-08-22T16:34:41.648861",
     "exception": false,
     "start_time": "2025-08-22T16:34:41.623059",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, TerminateOnNaN\n",
    "import numpy as np\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy, MeanSquaredError\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "if db_name == 'BCICIV2a':\n",
    "  db = Dataset_2a('/kaggle/input/bciiv2a-gcpds/BCI_CIV_2a')\n",
    "  fs = db.metadata['sampling_rate']\n",
    "  load_args = dict(db = db,\n",
    "                 fs = fs,\n",
    "                 f_bank = np.asarray([[4., 40.]]),\n",
    "                 vwt = np.asarray([[2.5, 6]]),\n",
    "                 new_fs = 128.)\n",
    "  subjects = np.arange(db.metadata['subjects']) + 1\n",
    "  \n",
    "elif db_name == 'GIGA_MI_ME':\n",
    "  db = GIGA_MI_ME('/kaggle/input/giga-science-gcpds/GIGA_MI_ME')\n",
    "  fs = db.metadata['sampling_rate']\n",
    "\n",
    "  # eeg_ch_names = ['Fp1','Fpz','Fp2',\n",
    "  #                    'AF7','AF3','AFz','AF4','AF8',\n",
    "  #                   'F7','F5','F3','F1','Fz','F2','F4','F6','F8',\n",
    "  #                  'FT7','FC5','FC3','FC1','FCz','FC2','FC4','FC6','FT8',\n",
    "  #                   'T7','C5','C3','C1','Cz','C2','C4','C6','T8',\n",
    "  #                  'TP7','CP5','CP3','CP1','CPz','CP2','CP4','CP6','TP8',\n",
    "  #                   'P9','P7','P5','P3','P1','Pz','P2','P4','P6','P8','P10',\n",
    "  #                   'PO7','PO3','POz','PO4','PO8',\n",
    "  #                   'O1','Oz','O2',\n",
    "  #                   'Iz']\n",
    "\n",
    "  # eeg_ch_names = ['Fp1','Fp2',\n",
    "  #                  'AF3','AF4',\n",
    "  #                  'F7','F3','Fz','F4','F8',\n",
    "  #                  'FC5','FC1','FC2','FC6',\n",
    "  #                  'T7','C3','Cz','C4','T8',\n",
    "  #                  'CP5','CP1','CP2','CP6',\n",
    "  #                  'P7','P3','Pz','P4','P8',\n",
    "  #                  'PO3','PO4',\n",
    "  #                  'O1','Oz','O2']\n",
    "\n",
    "  # eeg_ch_names = ['Fp1','Fp2',\n",
    "  #                 'F7','F3','F4','F8',\n",
    "  #                 'T7','C3','C4','T8',\n",
    "  #                 'P7','P3','P4','P8',\n",
    "  #                 'O1','O2']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  eeg_ch_names = ['Fp1','Fp2',\n",
    "               'T7','C3','C4','T8',\n",
    "               'O1','O2']\n",
    "    \n",
    "\n",
    "\n",
    "  load_args = dict(db = db,\n",
    "                  eeg_ch_names = eeg_ch_names,\n",
    "                  fs = fs,\n",
    "                  f_bank = np.asarray([[4., 40.]]),\n",
    "                  vwt = np.asarray([[2.5, 5]]),\n",
    "                  new_fs = 128.)\n",
    "  subjects = np.arange(db.metadata['subjects']) + 1\n",
    "  subjects = np.delete(subjects, [28,33])\n",
    "  \n",
    "else:\n",
    "  raise ValueError('No valid database name')\n",
    "\n",
    "verbose = 0\n",
    "reduce_lr_on_plateau = ReduceLROnPlateau(monitor = 'loss', factor = 0.1, patience = 30, verbose = verbose, mode = 'min', min_delta = 0.01, min_lr = 0)\n",
    "terminate_on_nan = TerminateOnNaN()\n",
    "callbacks = [reduce_lr_on_plateau, terminate_on_nan]\n",
    "seed = 23\n",
    "\n",
    "cv_args = dict(cv = StratifiedShuffleSplit(n_splits = 5, test_size = 0.2, random_state = seed))\n",
    "\n",
    "compile_args = dict(loss = SparseCategoricalCrossentropy(), #['mse' , SparseCategoricalCrossentropy()]\n",
    "                    init_lr = 1e-2)\n",
    "                      \n",
    "fit_args = dict(epochs = 500,\n",
    "                verbose = verbose,\n",
    "                callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1938a830",
   "metadata": {
    "id": "ukhXifxzTaj9",
    "papermill": {
     "duration": 0.011442,
     "end_time": "2025-08-22T16:34:41.671997",
     "exception": false,
     "start_time": "2025-08-22T16:34:41.660555",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e389a9f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T16:34:41.696275Z",
     "iopub.status.busy": "2025-08-22T16:34:41.696020Z",
     "iopub.status.idle": "2025-08-22T19:02:24.783134Z",
     "shell.execute_reply": "2025-08-22T19:02:24.782219Z"
    },
    "id": "Ymqd_W21y3NK",
    "outputId": "5ca97a2f-f57c-46ee-8f53-f00181ccea90",
    "papermill": {
     "duration": 8863.101491,
     "end_time": "2025-08-22T19:02:24.785124",
     "exception": false,
     "start_time": "2025-08-22T16:34:41.683633",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sbj =  8\n",
      "Filtered from 264 trials to 132 trials (classes 0 and 1 only).\n",
      "Resampling from 250.000000 to 128.000000 Hz.\n",
      "Filtered from 271 trials to 134 trials (classes 0 and 1 only).\n",
      "Resampling from 250.000000 to 128.000000 Hz.\n",
      "X_train shape: (266, 22, 448, 1), y_train shape: (266,)\n",
      "Running fold 0 with 212 training samples and 54 validation samples\n",
      "Fold 0: train indices: [113  90  13  10 187], val indices: [241  14 177 143  71]\n",
      "Training data shape: (212, 22, 448, 1), Validation data shape: (54, 22, 448, 1)\n",
      "Fold 0 training loss: 0.0030608077067881823\n",
      "y_true (val): [1 1 1 1 0]\n",
      "y_pred: [1 1 1 1 1]\n",
      "Appending results for fold 0: {'fold_index': 0, 'train_indices': [113, 90, 13, 10, 187, 185, 137, 12, 60, 171, 237, 244, 77, 216, 102, 32, 22, 254, 110, 50, 239, 121, 249, 206, 70, 165, 105, 93, 172, 7, 256, 80, 109, 229, 240, 24, 66, 5, 144, 116, 139, 123, 97, 265, 58, 154, 45, 194, 124, 35, 19, 223, 106, 168, 199, 210, 221, 214, 69, 79, 178, 18, 209, 260, 54, 128, 31, 1, 48, 17, 198, 164, 37, 227, 242, 38, 27, 251, 83, 175, 259, 134, 34, 245, 235, 75, 155, 49, 258, 115, 158, 41, 76, 211, 140, 247, 56, 16, 146, 85, 184, 248, 78, 138, 225, 9, 205, 36, 25, 150, 0, 212, 202, 82, 201, 43, 86, 111, 120, 218, 40, 195, 59, 81, 108, 219, 182, 74, 162, 26, 101, 98, 152, 252, 42, 20, 151, 122, 57, 232, 130, 3, 145, 224, 127, 243, 95, 228, 203, 204, 220, 96, 53, 188, 183, 230, 262, 191, 142, 160, 2, 255, 246, 181, 64, 153, 61, 170, 118, 114, 193, 222, 62, 189, 23, 87, 11, 207, 131, 197, 68, 67, 89, 39, 4, 136, 253, 47, 166, 84, 233, 159, 186, 149, 148, 44, 51, 231, 226, 133, 8, 238, 208, 263, 192, 29, 104, 156, 264, 33, 196, 100], 'val_indices': [241, 14, 177, 143, 71, 200, 107, 179, 157, 15, 250, 72, 103, 52, 141, 163, 73, 92, 94, 129, 63, 112, 176, 126, 161, 119, 30, 55, 261, 236, 46, 234, 21, 28, 213, 117, 147, 88, 174, 180, 91, 167, 65, 132, 135, 169, 190, 6, 99, 173, 217, 257, 125, 215], 'accuracy': 0.9444444444444444, 'kappa': 0.8888888888888888, 'auc': 0.9917695473251028}\n",
      "New Max Found!\n",
      "Running fold 1 with 212 training samples and 54 validation samples\n",
      "Fold 1: train indices: [102 149 115 152 196], val indices: [ 79 129 116 208 212]\n",
      "Training data shape: (212, 22, 448, 1), Validation data shape: (54, 22, 448, 1)\n",
      "Fold 1 training loss: 0.0029672575183212757\n",
      "y_true (val): [1 1 0 0 1]\n",
      "y_pred: [1 1 0 0 1]\n",
      "Appending results for fold 1: {'fold_index': 1, 'train_indices': [102, 149, 115, 152, 196, 44, 76, 101, 13, 66, 53, 187, 139, 24, 182, 130, 70, 33, 3, 61, 252, 257, 217, 224, 154, 8, 100, 177, 137, 27, 122, 98, 43, 73, 188, 172, 222, 35, 85, 65, 167, 213, 90, 50, 243, 69, 190, 263, 221, 193, 30, 251, 189, 106, 55, 165, 145, 229, 82, 146, 103, 20, 169, 223, 6, 114, 39, 219, 200, 51, 183, 84, 18, 148, 81, 176, 133, 107, 202, 126, 239, 242, 163, 134, 236, 67, 28, 209, 147, 231, 64, 164, 259, 5, 171, 232, 206, 185, 31, 195, 234, 111, 204, 15, 75, 38, 113, 155, 128, 237, 191, 119, 210, 260, 32, 77, 108, 226, 156, 256, 12, 248, 87, 153, 125, 46, 255, 161, 118, 47, 238, 112, 74, 160, 179, 41, 250, 168, 199, 258, 123, 151, 52, 29, 45, 72, 23, 132, 244, 22, 162, 104, 11, 186, 49, 97, 34, 214, 144, 216, 136, 254, 117, 96, 26, 48, 194, 124, 10, 88, 143, 235, 16, 37, 86, 253, 249, 205, 159, 218, 93, 91, 127, 58, 7, 207, 166, 241, 262, 201, 120, 225, 184, 57, 42, 109, 245, 138, 36, 180, 62, 141, 0, 215, 211, 175, 174, 246, 1, 173, 54, 80], 'val_indices': [79, 129, 116, 208, 212, 60, 142, 71, 261, 2, 135, 40, 94, 121, 228, 110, 227, 9, 131, 158, 203, 198, 59, 78, 265, 240, 181, 63, 220, 17, 4, 83, 170, 14, 192, 178, 95, 197, 99, 247, 150, 21, 233, 157, 68, 264, 230, 19, 56, 25, 92, 105, 140, 89], 'accuracy': 0.9629629629629629, 'kappa': 0.9259259259259259, 'auc': 1.0}\n",
      "New Max Found!\n",
      "Running fold 2 with 212 training samples and 54 validation samples\n",
      "Fold 2: train indices: [ 60 150  64 231  52], val indices: [189 184 182  88 191]\n",
      "Training data shape: (212, 22, 448, 1), Validation data shape: (54, 22, 448, 1)\n",
      "Fold 2 training loss: 0.0009892171947285533\n",
      "y_true (val): [0 0 0 0 1]\n",
      "y_pred: [0 0 0 0 1]\n",
      "Appending results for fold 2: {'fold_index': 2, 'train_indices': [60, 150, 64, 231, 52, 89, 200, 213, 237, 135, 136, 3, 40, 29, 15, 209, 212, 229, 154, 6, 113, 196, 9, 101, 47, 4, 188, 57, 178, 226, 137, 129, 224, 98, 153, 126, 148, 11, 38, 167, 192, 65, 175, 90, 187, 27, 69, 210, 262, 99, 131, 56, 164, 26, 110, 1, 157, 173, 19, 102, 20, 193, 25, 240, 143, 118, 165, 48, 138, 7, 58, 220, 67, 161, 84, 219, 103, 140, 250, 183, 159, 251, 211, 171, 194, 49, 141, 132, 166, 97, 214, 119, 50, 232, 16, 94, 142, 92, 73, 77, 81, 107, 24, 152, 122, 33, 176, 263, 180, 243, 86, 104, 125, 13, 39, 53, 156, 87, 163, 134, 235, 133, 201, 264, 246, 116, 63, 30, 32, 75, 244, 260, 37, 72, 46, 127, 109, 162, 121, 55, 221, 254, 185, 115, 45, 238, 218, 181, 258, 128, 83, 28, 242, 76, 239, 117, 179, 61, 111, 202, 177, 206, 31, 234, 190, 18, 227, 265, 261, 68, 74, 22, 256, 215, 216, 34, 105, 170, 51, 259, 43, 2, 35, 197, 21, 91, 186, 5, 230, 108, 217, 14, 80, 151, 225, 70, 106, 199, 174, 155, 146, 253, 257, 245, 139, 12, 233, 54, 23, 120, 145, 114], 'val_indices': [189, 184, 182, 88, 191, 62, 71, 10, 112, 241, 208, 79, 44, 96, 203, 85, 252, 222, 59, 36, 42, 0, 255, 169, 249, 82, 207, 247, 93, 228, 160, 8, 66, 95, 248, 223, 204, 17, 130, 195, 78, 168, 147, 236, 123, 158, 124, 198, 172, 149, 41, 144, 100, 205], 'accuracy': 0.9629629629629629, 'kappa': 0.9259259259259259, 'auc': 0.9945130315500685}\n",
      "Running fold 3 with 212 training samples and 54 validation samples\n",
      "Fold 3: train indices: [102 116 202 260 195], val indices: [  1  33 169  74  16]\n",
      "Training data shape: (212, 22, 448, 1), Validation data shape: (54, 22, 448, 1)\n",
      "Fold 3 training loss: 0.004005216993391514\n",
      "y_true (val): [0 1 1 0 1]\n",
      "y_pred: [0 1 0 0 1]\n",
      "Appending results for fold 3: {'fold_index': 3, 'train_indices': [102, 116, 202, 260, 195, 183, 84, 245, 101, 209, 18, 119, 24, 163, 75, 210, 108, 258, 106, 43, 208, 144, 4, 19, 99, 104, 59, 122, 220, 12, 92, 211, 263, 137, 160, 241, 35, 162, 193, 149, 53, 95, 227, 42, 117, 90, 112, 242, 257, 216, 148, 173, 233, 182, 125, 155, 52, 55, 71, 3, 150, 135, 41, 78, 224, 232, 207, 219, 111, 146, 67, 264, 170, 161, 103, 15, 185, 151, 197, 23, 181, 69, 188, 47, 198, 83, 115, 221, 109, 34, 8, 113, 184, 192, 248, 130, 48, 222, 231, 225, 14, 191, 79, 30, 175, 51, 10, 132, 37, 177, 247, 77, 80, 82, 176, 25, 46, 215, 180, 124, 121, 68, 54, 147, 114, 187, 140, 49, 32, 262, 154, 166, 139, 88, 203, 100, 31, 153, 110, 94, 174, 66, 265, 254, 27, 152, 261, 61, 196, 85, 76, 142, 229, 70, 217, 201, 186, 230, 237, 246, 214, 205, 45, 11, 93, 143, 249, 206, 131, 118, 190, 133, 44, 234, 39, 81, 194, 17, 96, 22, 212, 255, 21, 252, 62, 6, 165, 167, 235, 126, 91, 40, 129, 179, 226, 228, 57, 123, 97, 98, 87, 200, 199, 7, 136, 107, 253, 223, 138, 134, 89, 244], 'val_indices': [1, 33, 169, 74, 16, 251, 243, 158, 239, 128, 65, 63, 189, 56, 145, 157, 0, 36, 60, 13, 178, 156, 127, 159, 38, 250, 28, 218, 256, 64, 164, 86, 29, 20, 120, 72, 58, 5, 259, 50, 26, 141, 240, 238, 236, 2, 172, 171, 9, 105, 204, 168, 213, 73], 'accuracy': 0.9629629629629629, 'kappa': 0.9259259259259259, 'auc': 0.9972565157750343}\n",
      "Running fold 4 with 212 training samples and 54 validation samples\n",
      "Fold 4: train indices: [ 73 237   3  30 238], val indices: [242 256  72 254 101]\n",
      "Training data shape: (212, 22, 448, 1), Validation data shape: (54, 22, 448, 1)\n",
      "Fold 4 training loss: 0.0024529509246349335\n",
      "y_true (val): [1 1 0 0 1]\n",
      "y_pred: [1 1 0 0 1]\n",
      "Appending results for fold 4: {'fold_index': 4, 'train_indices': [73, 237, 3, 30, 238, 119, 69, 164, 122, 152, 159, 86, 60, 32, 34, 103, 166, 109, 16, 54, 216, 50, 17, 9, 145, 114, 176, 6, 95, 115, 61, 170, 157, 150, 8, 91, 75, 182, 211, 43, 94, 234, 44, 226, 249, 217, 161, 53, 107, 92, 0, 194, 223, 230, 58, 71, 133, 144, 155, 68, 55, 232, 112, 126, 240, 250, 25, 252, 221, 136, 31, 233, 190, 113, 48, 77, 243, 18, 200, 149, 89, 189, 188, 142, 224, 193, 108, 56, 5, 167, 1, 74, 203, 208, 80, 64, 236, 127, 210, 62, 7, 87, 225, 15, 195, 111, 229, 251, 175, 79, 137, 158, 4, 212, 52, 187, 39, 263, 207, 177, 12, 105, 98, 41, 201, 100, 131, 78, 181, 138, 143, 192, 171, 197, 42, 128, 255, 45, 196, 124, 57, 153, 24, 169, 120, 231, 246, 245, 11, 47, 185, 10, 264, 259, 26, 172, 19, 129, 106, 135, 198, 116, 110, 262, 36, 227, 35, 141, 146, 99, 220, 96, 20, 29, 247, 49, 121, 261, 82, 125, 199, 214, 202, 147, 102, 85, 104, 40, 38, 162, 97, 253, 239, 156, 191, 66, 140, 184, 117, 51, 186, 14, 70, 160, 204, 244, 67, 213, 260, 228, 180, 123], 'val_indices': [242, 256, 72, 254, 101, 222, 93, 174, 218, 88, 219, 173, 205, 21, 59, 178, 134, 33, 2, 241, 183, 215, 206, 148, 168, 63, 179, 46, 23, 90, 235, 65, 130, 27, 132, 248, 22, 257, 37, 83, 84, 76, 265, 154, 258, 151, 209, 163, 13, 118, 81, 139, 165, 28], 'accuracy': 0.9814814814814815, 'kappa': 0.962962962962963, 'auc': 0.9986282578875172}\n",
      "New Max Found!\n",
      "Final cross-validation results: {'params': [], 'mean_acc': 0.963, 'mean_kappa': 0.926, 'mean_auc': 0.996, 'mean_f1_left': array([0., 0., 0., 0., 0.]), 'mean_f1_right': array([0., 0., 0., 0., 0.]), 'mean_recall_left': array([0., 0., 0., 0., 0.]), 'mean_recall_right': array([0., 0., 0., 0., 0.]), 'mean_precision_left': array([0., 0., 0., 0., 0.]), 'mean_precision_right': array([0., 0., 0., 0., 0.]), 'all_folds': [{'fold_index': 0, 'train_indices': [113, 90, 13, 10, 187, 185, 137, 12, 60, 171, 237, 244, 77, 216, 102, 32, 22, 254, 110, 50, 239, 121, 249, 206, 70, 165, 105, 93, 172, 7, 256, 80, 109, 229, 240, 24, 66, 5, 144, 116, 139, 123, 97, 265, 58, 154, 45, 194, 124, 35, 19, 223, 106, 168, 199, 210, 221, 214, 69, 79, 178, 18, 209, 260, 54, 128, 31, 1, 48, 17, 198, 164, 37, 227, 242, 38, 27, 251, 83, 175, 259, 134, 34, 245, 235, 75, 155, 49, 258, 115, 158, 41, 76, 211, 140, 247, 56, 16, 146, 85, 184, 248, 78, 138, 225, 9, 205, 36, 25, 150, 0, 212, 202, 82, 201, 43, 86, 111, 120, 218, 40, 195, 59, 81, 108, 219, 182, 74, 162, 26, 101, 98, 152, 252, 42, 20, 151, 122, 57, 232, 130, 3, 145, 224, 127, 243, 95, 228, 203, 204, 220, 96, 53, 188, 183, 230, 262, 191, 142, 160, 2, 255, 246, 181, 64, 153, 61, 170, 118, 114, 193, 222, 62, 189, 23, 87, 11, 207, 131, 197, 68, 67, 89, 39, 4, 136, 253, 47, 166, 84, 233, 159, 186, 149, 148, 44, 51, 231, 226, 133, 8, 238, 208, 263, 192, 29, 104, 156, 264, 33, 196, 100], 'val_indices': [241, 14, 177, 143, 71, 200, 107, 179, 157, 15, 250, 72, 103, 52, 141, 163, 73, 92, 94, 129, 63, 112, 176, 126, 161, 119, 30, 55, 261, 236, 46, 234, 21, 28, 213, 117, 147, 88, 174, 180, 91, 167, 65, 132, 135, 169, 190, 6, 99, 173, 217, 257, 125, 215], 'accuracy': 0.9444444444444444, 'kappa': 0.8888888888888888, 'auc': 0.9917695473251028}, {'fold_index': 1, 'train_indices': [102, 149, 115, 152, 196, 44, 76, 101, 13, 66, 53, 187, 139, 24, 182, 130, 70, 33, 3, 61, 252, 257, 217, 224, 154, 8, 100, 177, 137, 27, 122, 98, 43, 73, 188, 172, 222, 35, 85, 65, 167, 213, 90, 50, 243, 69, 190, 263, 221, 193, 30, 251, 189, 106, 55, 165, 145, 229, 82, 146, 103, 20, 169, 223, 6, 114, 39, 219, 200, 51, 183, 84, 18, 148, 81, 176, 133, 107, 202, 126, 239, 242, 163, 134, 236, 67, 28, 209, 147, 231, 64, 164, 259, 5, 171, 232, 206, 185, 31, 195, 234, 111, 204, 15, 75, 38, 113, 155, 128, 237, 191, 119, 210, 260, 32, 77, 108, 226, 156, 256, 12, 248, 87, 153, 125, 46, 255, 161, 118, 47, 238, 112, 74, 160, 179, 41, 250, 168, 199, 258, 123, 151, 52, 29, 45, 72, 23, 132, 244, 22, 162, 104, 11, 186, 49, 97, 34, 214, 144, 216, 136, 254, 117, 96, 26, 48, 194, 124, 10, 88, 143, 235, 16, 37, 86, 253, 249, 205, 159, 218, 93, 91, 127, 58, 7, 207, 166, 241, 262, 201, 120, 225, 184, 57, 42, 109, 245, 138, 36, 180, 62, 141, 0, 215, 211, 175, 174, 246, 1, 173, 54, 80], 'val_indices': [79, 129, 116, 208, 212, 60, 142, 71, 261, 2, 135, 40, 94, 121, 228, 110, 227, 9, 131, 158, 203, 198, 59, 78, 265, 240, 181, 63, 220, 17, 4, 83, 170, 14, 192, 178, 95, 197, 99, 247, 150, 21, 233, 157, 68, 264, 230, 19, 56, 25, 92, 105, 140, 89], 'accuracy': 0.9629629629629629, 'kappa': 0.9259259259259259, 'auc': 1.0}, {'fold_index': 2, 'train_indices': [60, 150, 64, 231, 52, 89, 200, 213, 237, 135, 136, 3, 40, 29, 15, 209, 212, 229, 154, 6, 113, 196, 9, 101, 47, 4, 188, 57, 178, 226, 137, 129, 224, 98, 153, 126, 148, 11, 38, 167, 192, 65, 175, 90, 187, 27, 69, 210, 262, 99, 131, 56, 164, 26, 110, 1, 157, 173, 19, 102, 20, 193, 25, 240, 143, 118, 165, 48, 138, 7, 58, 220, 67, 161, 84, 219, 103, 140, 250, 183, 159, 251, 211, 171, 194, 49, 141, 132, 166, 97, 214, 119, 50, 232, 16, 94, 142, 92, 73, 77, 81, 107, 24, 152, 122, 33, 176, 263, 180, 243, 86, 104, 125, 13, 39, 53, 156, 87, 163, 134, 235, 133, 201, 264, 246, 116, 63, 30, 32, 75, 244, 260, 37, 72, 46, 127, 109, 162, 121, 55, 221, 254, 185, 115, 45, 238, 218, 181, 258, 128, 83, 28, 242, 76, 239, 117, 179, 61, 111, 202, 177, 206, 31, 234, 190, 18, 227, 265, 261, 68, 74, 22, 256, 215, 216, 34, 105, 170, 51, 259, 43, 2, 35, 197, 21, 91, 186, 5, 230, 108, 217, 14, 80, 151, 225, 70, 106, 199, 174, 155, 146, 253, 257, 245, 139, 12, 233, 54, 23, 120, 145, 114], 'val_indices': [189, 184, 182, 88, 191, 62, 71, 10, 112, 241, 208, 79, 44, 96, 203, 85, 252, 222, 59, 36, 42, 0, 255, 169, 249, 82, 207, 247, 93, 228, 160, 8, 66, 95, 248, 223, 204, 17, 130, 195, 78, 168, 147, 236, 123, 158, 124, 198, 172, 149, 41, 144, 100, 205], 'accuracy': 0.9629629629629629, 'kappa': 0.9259259259259259, 'auc': 0.9945130315500685}, {'fold_index': 3, 'train_indices': [102, 116, 202, 260, 195, 183, 84, 245, 101, 209, 18, 119, 24, 163, 75, 210, 108, 258, 106, 43, 208, 144, 4, 19, 99, 104, 59, 122, 220, 12, 92, 211, 263, 137, 160, 241, 35, 162, 193, 149, 53, 95, 227, 42, 117, 90, 112, 242, 257, 216, 148, 173, 233, 182, 125, 155, 52, 55, 71, 3, 150, 135, 41, 78, 224, 232, 207, 219, 111, 146, 67, 264, 170, 161, 103, 15, 185, 151, 197, 23, 181, 69, 188, 47, 198, 83, 115, 221, 109, 34, 8, 113, 184, 192, 248, 130, 48, 222, 231, 225, 14, 191, 79, 30, 175, 51, 10, 132, 37, 177, 247, 77, 80, 82, 176, 25, 46, 215, 180, 124, 121, 68, 54, 147, 114, 187, 140, 49, 32, 262, 154, 166, 139, 88, 203, 100, 31, 153, 110, 94, 174, 66, 265, 254, 27, 152, 261, 61, 196, 85, 76, 142, 229, 70, 217, 201, 186, 230, 237, 246, 214, 205, 45, 11, 93, 143, 249, 206, 131, 118, 190, 133, 44, 234, 39, 81, 194, 17, 96, 22, 212, 255, 21, 252, 62, 6, 165, 167, 235, 126, 91, 40, 129, 179, 226, 228, 57, 123, 97, 98, 87, 200, 199, 7, 136, 107, 253, 223, 138, 134, 89, 244], 'val_indices': [1, 33, 169, 74, 16, 251, 243, 158, 239, 128, 65, 63, 189, 56, 145, 157, 0, 36, 60, 13, 178, 156, 127, 159, 38, 250, 28, 218, 256, 64, 164, 86, 29, 20, 120, 72, 58, 5, 259, 50, 26, 141, 240, 238, 236, 2, 172, 171, 9, 105, 204, 168, 213, 73], 'accuracy': 0.9629629629629629, 'kappa': 0.9259259259259259, 'auc': 0.9972565157750343}, {'fold_index': 4, 'train_indices': [73, 237, 3, 30, 238, 119, 69, 164, 122, 152, 159, 86, 60, 32, 34, 103, 166, 109, 16, 54, 216, 50, 17, 9, 145, 114, 176, 6, 95, 115, 61, 170, 157, 150, 8, 91, 75, 182, 211, 43, 94, 234, 44, 226, 249, 217, 161, 53, 107, 92, 0, 194, 223, 230, 58, 71, 133, 144, 155, 68, 55, 232, 112, 126, 240, 250, 25, 252, 221, 136, 31, 233, 190, 113, 48, 77, 243, 18, 200, 149, 89, 189, 188, 142, 224, 193, 108, 56, 5, 167, 1, 74, 203, 208, 80, 64, 236, 127, 210, 62, 7, 87, 225, 15, 195, 111, 229, 251, 175, 79, 137, 158, 4, 212, 52, 187, 39, 263, 207, 177, 12, 105, 98, 41, 201, 100, 131, 78, 181, 138, 143, 192, 171, 197, 42, 128, 255, 45, 196, 124, 57, 153, 24, 169, 120, 231, 246, 245, 11, 47, 185, 10, 264, 259, 26, 172, 19, 129, 106, 135, 198, 116, 110, 262, 36, 227, 35, 141, 146, 99, 220, 96, 20, 29, 247, 49, 121, 261, 82, 125, 199, 214, 202, 147, 102, 85, 104, 40, 38, 162, 97, 253, 239, 156, 191, 66, 140, 184, 117, 51, 186, 14, 70, 160, 204, 244, 67, 213, 260, 228, 180, 123], 'val_indices': [242, 256, 72, 254, 101, 222, 93, 174, 218, 88, 219, 173, 205, 21, 59, 178, 134, 33, 2, 241, 183, 215, 206, 148, 168, 63, 179, 46, 23, 90, 235, 65, 130, 27, 132, 248, 22, 257, 37, 83, 84, 76, 265, 154, 258, 151, 209, 163, 13, 118, 81, 139, 165, 28], 'accuracy': 0.9814814814814815, 'kappa': 0.962962962962963, 'auc': 0.9986282578875172}], 'std_acc': 0.012, 'std_kappa': 0.023, 'std_auc': 0.003}\n",
      "sbj =  9\n",
      "Filtered from 237 trials to 116 trials (classes 0 and 1 only).\n",
      "Resampling from 250.000000 to 128.000000 Hz.\n",
      "Filtered from 264 trials to 130 trials (classes 0 and 1 only).\n",
      "Resampling from 250.000000 to 128.000000 Hz.\n",
      "X_train shape: (246, 22, 448, 1), y_train shape: (246,)\n",
      "Running fold 0 with 196 training samples and 50 validation samples\n",
      "Fold 0: train indices: [131 102  30 133 152], val indices: [ 12 142 101 109  81]\n",
      "Training data shape: (196, 22, 448, 1), Validation data shape: (50, 22, 448, 1)\n",
      "Fold 0 training loss: 0.012755966745316982\n",
      "y_true (val): [1 0 0 1 0]\n",
      "y_pred: [1 0 0 1 0]\n",
      "Appending results for fold 0: {'fold_index': 0, 'train_indices': [131, 102, 30, 133, 152, 84, 154, 64, 245, 106, 122, 203, 195, 153, 231, 123, 143, 9, 2, 157, 155, 76, 54, 5, 212, 165, 65, 159, 42, 189, 160, 36, 162, 150, 124, 53, 182, 151, 18, 83, 62, 238, 116, 225, 241, 180, 43, 19, 163, 215, 16, 235, 234, 132, 174, 41, 73, 96, 77, 21, 63, 104, 91, 60, 38, 243, 26, 198, 37, 34, 25, 46, 44, 136, 57, 158, 107, 187, 0, 4, 137, 128, 120, 204, 236, 240, 108, 149, 59, 207, 172, 110, 51, 66, 28, 58, 126, 181, 156, 45, 178, 72, 125, 192, 171, 70, 129, 23, 223, 1, 48, 79, 233, 145, 244, 216, 39, 175, 147, 201, 196, 85, 130, 194, 227, 113, 55, 94, 11, 52, 167, 121, 205, 140, 141, 56, 199, 164, 229, 78, 50, 206, 214, 190, 93, 191, 14, 144, 75, 105, 89, 232, 188, 138, 169, 210, 222, 118, 197, 3, 111, 27, 74, 80, 90, 134, 88, 95, 173, 224, 217, 119, 29, 183, 40, 202, 99, 31, 20, 213, 112, 193, 230, 7, 100, 98, 242, 228, 35, 179, 166, 184, 177, 67, 22, 176], 'val_indices': [12, 142, 101, 109, 81, 239, 47, 135, 208, 69, 226, 170, 32, 92, 209, 218, 6, 127, 161, 148, 15, 17, 211, 221, 115, 8, 114, 71, 168, 68, 61, 103, 87, 86, 13, 97, 117, 10, 146, 219, 185, 186, 237, 200, 220, 33, 139, 24, 49, 82], 'accuracy': 0.94, 'kappa': 0.8803827751196172, 'auc': 0.9967948717948718}\n",
      "New Max Found!\n",
      "Running fold 1 with 196 training samples and 50 validation samples\n",
      "Fold 1: train indices: [221 168 218 120  84], val indices: [150  50  63  71  39]\n",
      "Training data shape: (196, 22, 448, 1), Validation data shape: (50, 22, 448, 1)\n",
      "Fold 1 training loss: 0.0070394491776824\n",
      "y_true (val): [1 1 0 1 0]\n",
      "y_pred: [1 1 0 1 0]\n",
      "Appending results for fold 1: {'fold_index': 1, 'train_indices': [221, 168, 218, 120, 84, 57, 81, 189, 124, 98, 186, 75, 82, 11, 72, 143, 198, 3, 199, 223, 105, 70, 129, 239, 27, 156, 95, 24, 113, 225, 117, 212, 87, 107, 238, 192, 169, 173, 88, 226, 94, 185, 133, 128, 233, 132, 42, 18, 43, 130, 64, 16, 134, 2, 65, 86, 37, 231, 76, 222, 201, 49, 56, 53, 54, 191, 68, 228, 13, 162, 139, 106, 121, 55, 52, 46, 78, 214, 184, 77, 10, 146, 151, 100, 243, 44, 220, 187, 154, 122, 51, 165, 74, 188, 90, 34, 211, 194, 158, 204, 127, 45, 47, 176, 136, 38, 4, 171, 193, 145, 91, 164, 112, 108, 36, 235, 202, 182, 114, 234, 131, 62, 155, 102, 159, 60, 109, 245, 28, 141, 7, 224, 230, 61, 83, 206, 236, 104, 116, 138, 161, 30, 147, 196, 26, 200, 5, 170, 6, 244, 229, 152, 31, 58, 125, 73, 183, 32, 237, 219, 110, 180, 153, 208, 240, 99, 149, 166, 160, 97, 48, 35, 80, 142, 25, 175, 190, 22, 85, 17, 21, 89, 197, 135, 59, 8, 118, 29, 103, 215, 20, 209, 207, 41, 179, 216], 'val_indices': [150, 50, 63, 71, 39, 33, 174, 119, 213, 0, 15, 96, 232, 115, 79, 69, 242, 195, 241, 137, 205, 140, 167, 12, 123, 1, 101, 203, 9, 181, 92, 66, 172, 40, 67, 23, 93, 111, 14, 157, 227, 126, 210, 148, 217, 163, 178, 177, 19, 144], 'accuracy': 0.94, 'kappa': 0.88, 'auc': 0.971153846153846}\n",
      "Running fold 2 with 196 training samples and 50 validation samples\n",
      "Fold 2: train indices: [ 63 104 146  49  48], val indices: [140  93 237  88 242]\n",
      "Training data shape: (196, 22, 448, 1), Validation data shape: (50, 22, 448, 1)\n",
      "Fold 2 training loss: 0.004416508134454489\n",
      "y_true (val): [0 1 1 1 1]\n",
      "y_pred: [0 1 1 0 1]\n",
      "Appending results for fold 2: {'fold_index': 2, 'train_indices': [63, 104, 146, 49, 48, 67, 196, 8, 82, 47, 165, 41, 244, 232, 174, 37, 158, 40, 184, 240, 74, 151, 118, 43, 57, 215, 45, 62, 210, 177, 120, 9, 178, 220, 226, 70, 102, 96, 12, 175, 217, 99, 1, 6, 33, 145, 66, 72, 124, 76, 34, 230, 3, 23, 213, 5, 32, 212, 84, 97, 149, 122, 77, 112, 161, 36, 52, 229, 107, 60, 54, 194, 27, 65, 61, 154, 156, 95, 137, 103, 105, 42, 201, 222, 101, 75, 157, 214, 125, 127, 186, 187, 236, 0, 173, 136, 30, 19, 192, 189, 100, 17, 200, 138, 228, 73, 20, 131, 51, 135, 7, 116, 110, 238, 14, 181, 179, 197, 109, 91, 90, 108, 166, 53, 21, 38, 202, 245, 139, 191, 168, 56, 121, 172, 204, 234, 132, 221, 128, 241, 141, 227, 24, 235, 111, 205, 55, 155, 39, 69, 119, 195, 190, 169, 86, 207, 170, 92, 218, 199, 153, 126, 216, 206, 211, 160, 223, 25, 15, 71, 83, 233, 89, 22, 87, 152, 78, 28, 94, 225, 182, 29, 188, 224, 64, 185, 44, 180, 81, 58, 35, 31, 123, 2, 50, 203], 'val_indices': [140, 93, 237, 88, 242, 114, 231, 143, 142, 133, 159, 129, 134, 176, 162, 13, 18, 198, 219, 79, 59, 164, 208, 115, 243, 46, 163, 98, 144, 239, 11, 4, 150, 106, 148, 80, 209, 10, 171, 117, 113, 147, 68, 85, 167, 130, 16, 193, 26, 183], 'accuracy': 0.92, 'kappa': 0.8402555910543131, 'auc': 0.9775641025641026}\n",
      "Running fold 3 with 196 training samples and 50 validation samples\n",
      "Fold 3: train indices: [ 10 229 129   5 104], val indices: [ 14 196 183  26 143]\n",
      "Training data shape: (196, 22, 448, 1), Validation data shape: (50, 22, 448, 1)\n",
      "Fold 3 training loss: 0.001584001467563212\n",
      "y_true (val): [1 1 0 0 0]\n",
      "y_pred: [1 0 0 0 0]\n",
      "Appending results for fold 3: {'fold_index': 3, 'train_indices': [10, 229, 129, 5, 104, 60, 172, 169, 205, 24, 93, 243, 28, 210, 1, 241, 95, 140, 149, 144, 199, 114, 221, 233, 15, 50, 216, 202, 81, 76, 86, 78, 73, 127, 212, 195, 123, 32, 101, 213, 12, 126, 106, 64, 58, 220, 59, 222, 166, 135, 68, 231, 98, 47, 45, 79, 27, 67, 8, 223, 49, 188, 237, 215, 207, 132, 206, 57, 56, 112, 118, 214, 174, 162, 142, 159, 42, 187, 133, 108, 39, 66, 150, 145, 234, 193, 96, 121, 83, 63, 33, 122, 43, 173, 190, 141, 117, 30, 161, 105, 136, 185, 20, 192, 240, 242, 178, 228, 111, 176, 152, 179, 181, 74, 0, 22, 209, 163, 110, 53, 184, 137, 77, 82, 2, 4, 35, 197, 97, 99, 46, 236, 238, 175, 72, 34, 103, 194, 7, 244, 201, 100, 153, 239, 75, 232, 31, 198, 218, 217, 147, 139, 51, 89, 208, 44, 17, 191, 65, 156, 113, 200, 21, 92, 107, 180, 19, 128, 88, 125, 170, 177, 116, 52, 91, 120, 3, 219, 230, 29, 227, 151, 6, 224, 16, 71, 225, 124, 18, 102, 11, 148, 94, 203, 61, 134], 'val_indices': [14, 196, 183, 26, 143, 54, 85, 38, 37, 131, 48, 36, 40, 182, 70, 13, 204, 171, 155, 130, 138, 55, 167, 245, 226, 119, 115, 9, 164, 211, 25, 23, 69, 90, 109, 189, 165, 154, 80, 146, 235, 186, 62, 158, 168, 87, 160, 41, 157, 84], 'accuracy': 0.88, 'kappa': 0.7611464968152866, 'auc': 0.9599358974358975}\n",
      "Running fold 4 with 196 training samples and 50 validation samples\n",
      "Fold 4: train indices: [ 23  79  56 183 187], val indices: [ 26 234 144  15 191]\n",
      "Training data shape: (196, 22, 448, 1), Validation data shape: (50, 22, 448, 1)\n",
      "Fold 4 training loss: 0.001455370569601655\n",
      "y_true (val): [0 0 0 1 0]\n",
      "y_pred: [0 0 0 1 0]\n",
      "Appending results for fold 4: {'fold_index': 4, 'train_indices': [23, 79, 56, 183, 187, 65, 181, 232, 221, 54, 174, 48, 168, 143, 81, 42, 4, 140, 24, 146, 104, 99, 71, 178, 66, 190, 127, 89, 96, 94, 194, 201, 11, 164, 215, 188, 22, 184, 40, 142, 105, 95, 72, 210, 62, 145, 213, 182, 45, 136, 98, 20, 60, 5, 230, 195, 83, 50, 116, 30, 123, 216, 97, 0, 171, 68, 76, 28, 88, 154, 34, 243, 18, 52, 227, 172, 33, 55, 36, 202, 114, 197, 237, 208, 229, 64, 84, 32, 203, 204, 14, 156, 21, 74, 176, 225, 222, 85, 137, 126, 119, 117, 100, 133, 82, 214, 10, 111, 205, 196, 233, 44, 121, 151, 189, 193, 90, 166, 63, 69, 163, 12, 37, 17, 102, 80, 25, 139, 113, 9, 241, 226, 115, 235, 177, 150, 170, 186, 43, 138, 219, 134, 118, 173, 3, 16, 242, 75, 207, 159, 2, 165, 61, 109, 47, 162, 132, 7, 53, 131, 206, 59, 149, 41, 240, 220, 200, 147, 86, 92, 175, 129, 130, 91, 107, 155, 51, 13, 217, 29, 1, 223, 128, 67, 58, 106, 161, 73, 160, 158, 231, 35, 245, 49, 209, 239], 'val_indices': [26, 234, 144, 15, 191, 120, 108, 27, 212, 152, 39, 124, 180, 70, 46, 57, 148, 218, 6, 167, 77, 38, 112, 244, 31, 238, 185, 78, 19, 101, 87, 8, 211, 224, 93, 153, 110, 103, 125, 228, 141, 157, 135, 192, 179, 169, 199, 236, 122, 198], 'accuracy': 0.96, 'kappa': 0.9201277955271565, 'auc': 0.9967948717948718}\n",
      "New Max Found!\n",
      "Final cross-validation results: {'params': [], 'mean_acc': 0.928, 'mean_kappa': 0.856, 'mean_auc': 0.98, 'mean_f1_left': array([0., 0., 0., 0., 0.]), 'mean_f1_right': array([0., 0., 0., 0., 0.]), 'mean_recall_left': array([0., 0., 0., 0., 0.]), 'mean_recall_right': array([0., 0., 0., 0., 0.]), 'mean_precision_left': array([0., 0., 0., 0., 0.]), 'mean_precision_right': array([0., 0., 0., 0., 0.]), 'all_folds': [{'fold_index': 0, 'train_indices': [131, 102, 30, 133, 152, 84, 154, 64, 245, 106, 122, 203, 195, 153, 231, 123, 143, 9, 2, 157, 155, 76, 54, 5, 212, 165, 65, 159, 42, 189, 160, 36, 162, 150, 124, 53, 182, 151, 18, 83, 62, 238, 116, 225, 241, 180, 43, 19, 163, 215, 16, 235, 234, 132, 174, 41, 73, 96, 77, 21, 63, 104, 91, 60, 38, 243, 26, 198, 37, 34, 25, 46, 44, 136, 57, 158, 107, 187, 0, 4, 137, 128, 120, 204, 236, 240, 108, 149, 59, 207, 172, 110, 51, 66, 28, 58, 126, 181, 156, 45, 178, 72, 125, 192, 171, 70, 129, 23, 223, 1, 48, 79, 233, 145, 244, 216, 39, 175, 147, 201, 196, 85, 130, 194, 227, 113, 55, 94, 11, 52, 167, 121, 205, 140, 141, 56, 199, 164, 229, 78, 50, 206, 214, 190, 93, 191, 14, 144, 75, 105, 89, 232, 188, 138, 169, 210, 222, 118, 197, 3, 111, 27, 74, 80, 90, 134, 88, 95, 173, 224, 217, 119, 29, 183, 40, 202, 99, 31, 20, 213, 112, 193, 230, 7, 100, 98, 242, 228, 35, 179, 166, 184, 177, 67, 22, 176], 'val_indices': [12, 142, 101, 109, 81, 239, 47, 135, 208, 69, 226, 170, 32, 92, 209, 218, 6, 127, 161, 148, 15, 17, 211, 221, 115, 8, 114, 71, 168, 68, 61, 103, 87, 86, 13, 97, 117, 10, 146, 219, 185, 186, 237, 200, 220, 33, 139, 24, 49, 82], 'accuracy': 0.94, 'kappa': 0.8803827751196172, 'auc': 0.9967948717948718}, {'fold_index': 1, 'train_indices': [221, 168, 218, 120, 84, 57, 81, 189, 124, 98, 186, 75, 82, 11, 72, 143, 198, 3, 199, 223, 105, 70, 129, 239, 27, 156, 95, 24, 113, 225, 117, 212, 87, 107, 238, 192, 169, 173, 88, 226, 94, 185, 133, 128, 233, 132, 42, 18, 43, 130, 64, 16, 134, 2, 65, 86, 37, 231, 76, 222, 201, 49, 56, 53, 54, 191, 68, 228, 13, 162, 139, 106, 121, 55, 52, 46, 78, 214, 184, 77, 10, 146, 151, 100, 243, 44, 220, 187, 154, 122, 51, 165, 74, 188, 90, 34, 211, 194, 158, 204, 127, 45, 47, 176, 136, 38, 4, 171, 193, 145, 91, 164, 112, 108, 36, 235, 202, 182, 114, 234, 131, 62, 155, 102, 159, 60, 109, 245, 28, 141, 7, 224, 230, 61, 83, 206, 236, 104, 116, 138, 161, 30, 147, 196, 26, 200, 5, 170, 6, 244, 229, 152, 31, 58, 125, 73, 183, 32, 237, 219, 110, 180, 153, 208, 240, 99, 149, 166, 160, 97, 48, 35, 80, 142, 25, 175, 190, 22, 85, 17, 21, 89, 197, 135, 59, 8, 118, 29, 103, 215, 20, 209, 207, 41, 179, 216], 'val_indices': [150, 50, 63, 71, 39, 33, 174, 119, 213, 0, 15, 96, 232, 115, 79, 69, 242, 195, 241, 137, 205, 140, 167, 12, 123, 1, 101, 203, 9, 181, 92, 66, 172, 40, 67, 23, 93, 111, 14, 157, 227, 126, 210, 148, 217, 163, 178, 177, 19, 144], 'accuracy': 0.94, 'kappa': 0.88, 'auc': 0.971153846153846}, {'fold_index': 2, 'train_indices': [63, 104, 146, 49, 48, 67, 196, 8, 82, 47, 165, 41, 244, 232, 174, 37, 158, 40, 184, 240, 74, 151, 118, 43, 57, 215, 45, 62, 210, 177, 120, 9, 178, 220, 226, 70, 102, 96, 12, 175, 217, 99, 1, 6, 33, 145, 66, 72, 124, 76, 34, 230, 3, 23, 213, 5, 32, 212, 84, 97, 149, 122, 77, 112, 161, 36, 52, 229, 107, 60, 54, 194, 27, 65, 61, 154, 156, 95, 137, 103, 105, 42, 201, 222, 101, 75, 157, 214, 125, 127, 186, 187, 236, 0, 173, 136, 30, 19, 192, 189, 100, 17, 200, 138, 228, 73, 20, 131, 51, 135, 7, 116, 110, 238, 14, 181, 179, 197, 109, 91, 90, 108, 166, 53, 21, 38, 202, 245, 139, 191, 168, 56, 121, 172, 204, 234, 132, 221, 128, 241, 141, 227, 24, 235, 111, 205, 55, 155, 39, 69, 119, 195, 190, 169, 86, 207, 170, 92, 218, 199, 153, 126, 216, 206, 211, 160, 223, 25, 15, 71, 83, 233, 89, 22, 87, 152, 78, 28, 94, 225, 182, 29, 188, 224, 64, 185, 44, 180, 81, 58, 35, 31, 123, 2, 50, 203], 'val_indices': [140, 93, 237, 88, 242, 114, 231, 143, 142, 133, 159, 129, 134, 176, 162, 13, 18, 198, 219, 79, 59, 164, 208, 115, 243, 46, 163, 98, 144, 239, 11, 4, 150, 106, 148, 80, 209, 10, 171, 117, 113, 147, 68, 85, 167, 130, 16, 193, 26, 183], 'accuracy': 0.92, 'kappa': 0.8402555910543131, 'auc': 0.9775641025641026}, {'fold_index': 3, 'train_indices': [10, 229, 129, 5, 104, 60, 172, 169, 205, 24, 93, 243, 28, 210, 1, 241, 95, 140, 149, 144, 199, 114, 221, 233, 15, 50, 216, 202, 81, 76, 86, 78, 73, 127, 212, 195, 123, 32, 101, 213, 12, 126, 106, 64, 58, 220, 59, 222, 166, 135, 68, 231, 98, 47, 45, 79, 27, 67, 8, 223, 49, 188, 237, 215, 207, 132, 206, 57, 56, 112, 118, 214, 174, 162, 142, 159, 42, 187, 133, 108, 39, 66, 150, 145, 234, 193, 96, 121, 83, 63, 33, 122, 43, 173, 190, 141, 117, 30, 161, 105, 136, 185, 20, 192, 240, 242, 178, 228, 111, 176, 152, 179, 181, 74, 0, 22, 209, 163, 110, 53, 184, 137, 77, 82, 2, 4, 35, 197, 97, 99, 46, 236, 238, 175, 72, 34, 103, 194, 7, 244, 201, 100, 153, 239, 75, 232, 31, 198, 218, 217, 147, 139, 51, 89, 208, 44, 17, 191, 65, 156, 113, 200, 21, 92, 107, 180, 19, 128, 88, 125, 170, 177, 116, 52, 91, 120, 3, 219, 230, 29, 227, 151, 6, 224, 16, 71, 225, 124, 18, 102, 11, 148, 94, 203, 61, 134], 'val_indices': [14, 196, 183, 26, 143, 54, 85, 38, 37, 131, 48, 36, 40, 182, 70, 13, 204, 171, 155, 130, 138, 55, 167, 245, 226, 119, 115, 9, 164, 211, 25, 23, 69, 90, 109, 189, 165, 154, 80, 146, 235, 186, 62, 158, 168, 87, 160, 41, 157, 84], 'accuracy': 0.88, 'kappa': 0.7611464968152866, 'auc': 0.9599358974358975}, {'fold_index': 4, 'train_indices': [23, 79, 56, 183, 187, 65, 181, 232, 221, 54, 174, 48, 168, 143, 81, 42, 4, 140, 24, 146, 104, 99, 71, 178, 66, 190, 127, 89, 96, 94, 194, 201, 11, 164, 215, 188, 22, 184, 40, 142, 105, 95, 72, 210, 62, 145, 213, 182, 45, 136, 98, 20, 60, 5, 230, 195, 83, 50, 116, 30, 123, 216, 97, 0, 171, 68, 76, 28, 88, 154, 34, 243, 18, 52, 227, 172, 33, 55, 36, 202, 114, 197, 237, 208, 229, 64, 84, 32, 203, 204, 14, 156, 21, 74, 176, 225, 222, 85, 137, 126, 119, 117, 100, 133, 82, 214, 10, 111, 205, 196, 233, 44, 121, 151, 189, 193, 90, 166, 63, 69, 163, 12, 37, 17, 102, 80, 25, 139, 113, 9, 241, 226, 115, 235, 177, 150, 170, 186, 43, 138, 219, 134, 118, 173, 3, 16, 242, 75, 207, 159, 2, 165, 61, 109, 47, 162, 132, 7, 53, 131, 206, 59, 149, 41, 240, 220, 200, 147, 86, 92, 175, 129, 130, 91, 107, 155, 51, 13, 217, 29, 1, 223, 128, 67, 58, 106, 161, 73, 160, 158, 231, 35, 245, 49, 209, 239], 'val_indices': [26, 234, 144, 15, 191, 120, 108, 27, 212, 152, 39, 124, 180, 70, 46, 57, 148, 218, 6, 167, 77, 38, 112, 244, 31, 238, 185, 78, 19, 101, 87, 8, 211, 224, 93, 153, 110, 103, 125, 228, 141, 157, 135, 192, 179, 169, 199, 236, 122, 198], 'accuracy': 0.96, 'kappa': 0.9201277955271565, 'auc': 0.9967948717948718}], 'std_acc': 0.027, 'std_kappa': 0.054, 'std_auc': 0.014}\n"
     ]
    }
   ],
   "source": [
    "from pickle import dump\n",
    "\n",
    "subjects = [8,9]\n",
    "\n",
    "for sbj in subjects[:]:\n",
    "  print('sbj = ', sbj)\n",
    "  load_args['sbj'] = sbj\n",
    "  results = train(db_name, load_args, cv_args, model_args, compile_args, fit_args, seed)\n",
    "  with open('sbj' + str(load_args['sbj']) + '.txt', 'wb') as f:\n",
    "    dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "336ed647",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T19:02:24.813607Z",
     "iopub.status.busy": "2025-08-22T19:02:24.813306Z",
     "iopub.status.idle": "2025-08-22T19:02:26.890932Z",
     "shell.execute_reply": "2025-08-22T19:02:26.890053Z"
    },
    "id": "V7-P0xjwzXVX",
    "outputId": "270dceef-351d-48d1-f71e-2c3367c7fdac",
    "papermill": {
     "duration": 2.093997,
     "end_time": "2025-08-22T19:02:26.892911",
     "exception": false,
     "start_time": "2025-08-22T19:02:24.798914",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: sbj8.h5 (deflated 67%)\r\n",
      "  adding: sbj9.h5 (deflated 67%)\r\n",
      "  adding: sbj8.txt (deflated 39%)\r\n",
      "  adding: sbj9.txt (deflated 39%)\r\n"
     ]
    }
   ],
   "source": [
    "!zip Models_8ch_EEGNet_BCI.zip ./*.h5 \n",
    "!zip Results_8ch_EEGNet_BCI.zip ./*.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eac84c29",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T19:02:26.921771Z",
     "iopub.status.busy": "2025-08-22T19:02:26.921483Z",
     "iopub.status.idle": "2025-08-22T19:02:26.925304Z",
     "shell.execute_reply": "2025-08-22T19:02:26.924640Z"
    },
    "papermill": {
     "duration": 0.019804,
     "end_time": "2025-08-22T19:02:26.926814",
     "exception": false,
     "start_time": "2025-08-22T19:02:26.907010",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import pickle as pkl\n",
    "\n",
    "#with open(file= '/kaggle/working/sbj14.txt', mode = 'rb' ) as f:\n",
    "#    results_64ch_ShallowConvNet = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b530b2c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T19:02:26.955436Z",
     "iopub.status.busy": "2025-08-22T19:02:26.954933Z",
     "iopub.status.idle": "2025-08-22T19:02:26.958673Z",
     "shell.execute_reply": "2025-08-22T19:02:26.957852Z"
    },
    "papermill": {
     "duration": 0.019683,
     "end_time": "2025-08-22T19:02:26.960249",
     "exception": false,
     "start_time": "2025-08-22T19:02:26.940566",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#results_64ch_ShallowConvNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40b75304",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T19:02:26.987744Z",
     "iopub.status.busy": "2025-08-22T19:02:26.987495Z",
     "iopub.status.idle": "2025-08-22T19:02:26.990753Z",
     "shell.execute_reply": "2025-08-22T19:02:26.990125Z"
    },
    "papermill": {
     "duration": 0.018871,
     "end_time": "2025-08-22T19:02:26.992349",
     "exception": false,
     "start_time": "2025-08-22T19:02:26.973478",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#with open(file= '/kaggle/working/sbj2.txt', mode = 'rb' ) as f:\n",
    " #   results_64ch_ShallowConvNet = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d6a29bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-22T19:02:27.019741Z",
     "iopub.status.busy": "2025-08-22T19:02:27.019489Z",
     "iopub.status.idle": "2025-08-22T19:02:27.022746Z",
     "shell.execute_reply": "2025-08-22T19:02:27.022129Z"
    },
    "papermill": {
     "duration": 0.018643,
     "end_time": "2025-08-22T19:02:27.024210",
     "exception": false,
     "start_time": "2025-08-22T19:02:27.005567",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#results_64ch_ShallowConvNet"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 1645904,
     "sourceId": 2702213,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 1269900,
     "sourceId": 2702226,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 2984453,
     "sourceId": 5137200,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3008205,
     "sourceId": 5175158,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3009745,
     "sourceId": 5177340,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 9019.544627,
   "end_time": "2025-08-22T19:02:29.941630",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-08-22T16:32:10.397003",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
