{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df8a0918",
   "metadata": {
    "id": "x9LNzEYERaH2",
    "papermill": {
     "duration": 0.004519,
     "end_time": "2025-05-21T13:22:09.577767",
     "exception": false,
     "start_time": "2025-05-21T13:22:09.573248",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Download Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "140a6278",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T13:22:09.587013Z",
     "iopub.status.busy": "2025-05-21T13:22:09.586631Z",
     "iopub.status.idle": "2025-05-21T13:22:10.688771Z",
     "shell.execute_reply": "2025-05-21T13:22:10.687527Z"
    },
    "papermill": {
     "duration": 1.109811,
     "end_time": "2025-05-21T13:22:10.691547",
     "exception": false,
     "start_time": "2025-05-21T13:22:09.581736",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: nvidia-smi: command not found\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e1d506e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T13:22:10.701458Z",
     "iopub.status.busy": "2025-05-21T13:22:10.701025Z",
     "iopub.status.idle": "2025-05-21T13:23:12.039686Z",
     "shell.execute_reply": "2025-05-21T13:23:12.038354Z"
    },
    "id": "K0oS6IH7VTZX",
    "papermill": {
     "duration": 61.346502,
     "end_time": "2025-05-21T13:23:12.042119",
     "exception": false,
     "start_time": "2025-05-21T13:22:10.695617",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -U git+https://github.com/UN-GCPDS/python-gcpds.databases #Package for database reading.\n",
    "!pip install mne #The MNE Package is installed\n",
    "FILEID = \"1lo0MjWLvsyne2CgTA6VZ2HGY9SKxiwZ7\"\n",
    "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id='$FILEID -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=\"$FILEID -O MI_EEG_ClassMeth.zip && rm -rf /tmp/cookies.txt\n",
    "!unzip MI_EEG_ClassMeth.zip #Package with useful functions for motor imagery classification based in EEG.\n",
    "!pip install -U git+https://github.com/UN-GCPDS/python-gcpds.EEG_Tensorflow_models.git\n",
    "!dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4186c6a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T13:23:12.052289Z",
     "iopub.status.busy": "2025-05-21T13:23:12.051760Z",
     "iopub.status.idle": "2025-05-21T13:28:15.881557Z",
     "shell.execute_reply": "2025-05-21T13:28:15.880089Z"
    },
    "papermill": {
     "duration": 303.838281,
     "end_time": "2025-05-21T13:28:15.884565",
     "exception": false,
     "start_time": "2025-05-21T13:23:12.046284",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\r\n",
      "\r\n",
      "E: Unable to locate package libcudnn8\r\n",
      "Collecting tensorflow==2.8.2\r\n",
      "  Downloading tensorflow-2.8.2-cp310-cp310-manylinux2010_x86_64.whl.metadata (2.9 kB)\r\n",
      "Requirement already satisfied: absl-py>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.8.2) (1.4.0)\r\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.8.2) (1.6.3)\r\n",
      "Requirement already satisfied: flatbuffers>=1.12 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.8.2) (24.3.25)\r\n",
      "Requirement already satisfied: gast>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.8.2) (0.5.4)\r\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.8.2) (0.2.0)\r\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.8.2) (3.11.0)\r\n",
      "Collecting keras-preprocessing>=1.1.1 (from tensorflow==2.8.2)\r\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl.metadata (1.9 kB)\r\n",
      "Requirement already satisfied: libclang>=9.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.8.2) (18.1.1)\r\n",
      "Requirement already satisfied: numpy>=1.20 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.8.2) (1.26.4)\r\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.8.2) (3.3.0)\r\n",
      "Collecting protobuf<3.20,>=3.9.2 (from tensorflow==2.8.2)\r\n",
      "  Downloading protobuf-3.19.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (787 bytes)\r\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.8.2) (70.0.0)\r\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.8.2) (1.16.0)\r\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.8.2) (2.4.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.8.2) (4.12.2)\r\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.8.2) (1.16.0)\r\n",
      "Collecting tensorboard<2.9,>=2.8 (from tensorflow==2.8.2)\r\n",
      "  Downloading tensorboard-2.8.0-py3-none-any.whl.metadata (1.9 kB)\r\n",
      "Collecting tensorflow-estimator<2.9,>=2.8 (from tensorflow==2.8.2)\r\n",
      "  Downloading tensorflow_estimator-2.8.0-py2.py3-none-any.whl.metadata (1.3 kB)\r\n",
      "Collecting keras<2.9,>=2.8.0rc0 (from tensorflow==2.8.2)\r\n",
      "  Downloading keras-2.8.0-py2.py3-none-any.whl.metadata (1.3 kB)\r\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.8.2) (0.37.0)\r\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow==2.8.2) (1.64.1)\r\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow==2.8.2) (0.43.0)\r\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.2) (2.30.0)\r\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.9,>=2.8->tensorflow==2.8.2)\r\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl.metadata (2.7 kB)\r\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.2) (3.6)\r\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.2) (2.32.3)\r\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard<2.9,>=2.8->tensorflow==2.8.2)\r\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl.metadata (1.1 kB)\r\n",
      "Collecting tensorboard-plugin-wit>=1.6.0 (from tensorboard<2.9,>=2.8->tensorflow==2.8.2)\r\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl.metadata (873 bytes)\r\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.2) (3.0.4)\r\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.2) (4.2.4)\r\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.2) (0.4.0)\r\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.2) (4.9)\r\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8.2) (2.0.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.2) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.2) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.2) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.2) (2024.8.30)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=0.11.15->tensorboard<2.9,>=2.8->tensorflow==2.8.2) (2.1.5)\r\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.2) (0.6.0)\r\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8.2) (3.2.2)\r\n",
      "Downloading tensorflow-2.8.2-cp310-cp310-manylinux2010_x86_64.whl (498.0 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m498.0/498.0 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading keras-2.8.0-py2.py3-none-any.whl (1.4 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading protobuf-3.19.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m68.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading tensorflow_estimator-2.8.0-py2.py3-none-any.whl (462 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.3/462.3 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\r\n",
      "Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m71.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: tensorflow-estimator, tensorboard-plugin-wit, keras, tensorboard-data-server, protobuf, keras-preprocessing, google-auth-oauthlib, tensorboard, tensorflow\r\n",
      "  Attempting uninstall: tensorflow-estimator\r\n",
      "    Found existing installation: tensorflow-estimator 2.15.0\r\n",
      "    Uninstalling tensorflow-estimator-2.15.0:\r\n",
      "      Successfully uninstalled tensorflow-estimator-2.15.0\r\n",
      "  Attempting uninstall: keras\r\n",
      "    Found existing installation: keras 3.3.3\r\n",
      "    Uninstalling keras-3.3.3:\r\n",
      "      Successfully uninstalled keras-3.3.3\r\n",
      "  Attempting uninstall: tensorboard-data-server\r\n",
      "    Found existing installation: tensorboard-data-server 0.7.2\r\n",
      "    Uninstalling tensorboard-data-server-0.7.2:\r\n",
      "      Successfully uninstalled tensorboard-data-server-0.7.2\r\n",
      "  Attempting uninstall: protobuf\r\n",
      "    Found existing installation: protobuf 3.20.3\r\n",
      "    Uninstalling protobuf-3.20.3:\r\n",
      "      Successfully uninstalled protobuf-3.20.3\r\n",
      "  Attempting uninstall: google-auth-oauthlib\r\n",
      "    Found existing installation: google-auth-oauthlib 1.2.0\r\n",
      "    Uninstalling google-auth-oauthlib-1.2.0:\r\n",
      "      Successfully uninstalled google-auth-oauthlib-1.2.0\r\n",
      "  Attempting uninstall: tensorboard\r\n",
      "    Found existing installation: tensorboard 2.16.2\r\n",
      "    Uninstalling tensorboard-2.16.2:\r\n",
      "      Successfully uninstalled tensorboard-2.16.2\r\n",
      "  Attempting uninstall: tensorflow\r\n",
      "    Found existing installation: tensorflow 2.16.1\r\n",
      "    Uninstalling tensorflow-2.16.1:\r\n",
      "      Successfully uninstalled tensorflow-2.16.1\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "apache-beam 2.46.0 requires cloudpickle~=2.2.1, but you have cloudpickle 3.0.0 which is incompatible.\r\n",
      "apache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\r\n",
      "apache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\r\n",
      "apache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 17.0.0 which is incompatible.\r\n",
      "google-ai-generativelanguage 0.6.10 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\r\n",
      "google-cloud-aiplatform 0.6.0a1 requires google-api-core[grpc]<2.0.0dev,>=1.22.2, but you have google-api-core 2.11.1 which is incompatible.\r\n",
      "google-cloud-automl 1.0.1 requires google-api-core[grpc]<2.0.0dev,>=1.14.0, but you have google-api-core 2.11.1 which is incompatible.\r\n",
      "google-cloud-language 2.14.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\r\n",
      "google-cloud-spanner 3.47.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\r\n",
      "google-cloud-videointelligence 2.13.5 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\r\n",
      "kfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\r\n",
      "onnx 1.17.0 requires protobuf>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\r\n",
      "tensorboardx 2.6.2.2 requires protobuf>=3.20, but you have protobuf 3.19.6 which is incompatible.\r\n",
      "tensorflow-datasets 4.9.6 requires protobuf>=3.20, but you have protobuf 3.19.6 which is incompatible.\r\n",
      "tensorflow-decision-forests 1.9.1 requires tensorflow~=2.16.1, but you have tensorflow 2.8.2 which is incompatible.\r\n",
      "tensorflow-serving-api 2.16.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 3.19.6 which is incompatible.\r\n",
      "tensorflow-serving-api 2.16.1 requires tensorflow<3,>=2.16.1, but you have tensorflow 2.8.2 which is incompatible.\r\n",
      "tensorflow-text 2.16.1 requires tensorflow<2.17,>=2.16.1; platform_machine != \"arm64\" or platform_system != \"Darwin\", but you have tensorflow 2.8.2 which is incompatible.\r\n",
      "tf-keras 2.16.0 requires tensorflow<2.17,>=2.16, but you have tensorflow 2.8.2 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed google-auth-oauthlib-0.4.6 keras-2.8.0 keras-preprocessing-1.1.2 protobuf-3.19.6 tensorboard-2.8.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.8.2 tensorflow-estimator-2.8.0\r\n"
     ]
    }
   ],
   "source": [
    "!apt-get install --allow-change-held-packages libcudnn8=8.1.0.77-1+cuda11.2 -y\n",
    "!pip install tensorflow==2.8.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76108bd",
   "metadata": {
    "papermill": {
     "duration": 0.247775,
     "end_time": "2025-05-21T13:28:16.380082",
     "exception": false,
     "start_time": "2025-05-21T13:28:16.132307",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c97341b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T13:28:17.002370Z",
     "iopub.status.busy": "2025-05-21T13:28:17.001680Z",
     "iopub.status.idle": "2025-05-21T13:28:29.609037Z",
     "shell.execute_reply": "2025-05-21T13:28:29.607722Z"
    },
    "id": "yE1sbHYQVbBq",
    "papermill": {
     "duration": 12.904799,
     "end_time": "2025-05-21T13:28:29.611760",
     "exception": false,
     "start_time": "2025-05-21T13:28:16.706961",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gcpds.databases.BCI_Competition_IV import Dataset_2a\n",
    "from typing import Sequence, Tuple\n",
    "from MI_EEG_ClassMeth.FeatExtraction import TimeFrequencyRpr\n",
    "import numpy as np\n",
    "from scipy.signal import resample\n",
    "\n",
    "def load_BCICIV2a(db: Dataset_2a,\n",
    "               sbj: int,\n",
    "               mode: str,\n",
    "               fs: float, \n",
    "               f_bank: np.ndarray, \n",
    "               vwt: np.ndarray, \n",
    "               new_fs: float) -> np.ndarray:\n",
    "\n",
    "  tf_repr = TimeFrequencyRpr(sfreq = fs, f_bank = f_bank, vwt = vwt)\n",
    "\n",
    "  db.load_subject(sbj, mode = mode)\n",
    "    \n",
    "  X, y = db.get_data() #Load all classes, all channels {EEG, EOG}, reject bad trials\n",
    "\n",
    "  X = X[:,:-3,:] # pick EEG channels\n",
    "  X = X*1e6 #uV\n",
    "  X = np.squeeze(tf_repr.transform(X))\n",
    "  #Resampling\n",
    "  if new_fs == fs:\n",
    "    print('No resampling, since new sampling rate same.')\n",
    "  else:\n",
    "    print(\"Resampling from {:f} to {:f} Hz.\".format(fs, new_fs))\n",
    "    X = resample(X, int((X.shape[-1]/fs)*new_fs), axis = -1)\n",
    "    \n",
    "  return X, y\n",
    "\n",
    "\n",
    "from gcpds.databases import GIGA_MI_ME\n",
    "\n",
    "def load_GIGA_MI_ME(db: GIGA_MI_ME,\n",
    "              sbj: int,\n",
    "              eeg_ch_names: Sequence[str],\n",
    "              fs: float, \n",
    "              f_bank: np.ndarray, \n",
    "              vwt: np.ndarray, \n",
    "              new_fs: float) -> Tuple[np.ndarray, np.ndarray]:\n",
    "\n",
    "    index_eeg_chs = db.format_channels_selectors(channels=eeg_ch_names) - 1\n",
    "\n",
    "    tf_repr = TimeFrequencyRpr(sfreq=fs, f_bank=f_bank, vwt=vwt)\n",
    "\n",
    "    # Load subject data\n",
    "    db.load_subject(sbj)\n",
    "    X, y = db.get_data(classes=['left hand mi', 'right hand mi'])\n",
    "    \n",
    "    # Debugging total trials\n",
    "    print(f\"Total trials loaded: {X.shape[0]}\")\n",
    "    print(f\"Shape of X: {X.shape}, Shape of y: {y.shape}\")\n",
    "\n",
    "    # Spatial rearrangement\n",
    "    X = X[:, index_eeg_chs, :]  \n",
    "    X = np.squeeze(tf_repr.transform(X))\n",
    "\n",
    "    # Resampling\n",
    "    if new_fs == fs:\n",
    "        print('No resampling, since new sampling rate is the same.')\n",
    "    else:\n",
    "        print(f\"Resampling from {fs} to {new_fs} Hz.\")\n",
    "        X = resample(X, int((X.shape[-1] / fs) * new_fs), axis=-1)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "\n",
    "\n",
    "def load_DB(db_name, **load_args):\n",
    "  if db_name == 'BCICIV2a':\n",
    "    X_train, y_train = load_BCICIV2a(**load_args, mode = 'training')\n",
    "    X_test, y_test = load_BCICIV2a(**load_args, mode = 'evaluation')\n",
    "\n",
    "    X_train = np.concatenate([X_train, X_test], axis = 0)\n",
    "    y_train = np.concatenate([y_train, y_test], axis = 0)\n",
    "\n",
    "  elif db_name == 'GIGA_MI_ME':\n",
    "    X_train, y_train = load_GIGA_MI_ME(**load_args)\n",
    "    \n",
    "  else:\n",
    "    raise ValueError('No valid database name')\n",
    "\n",
    "  return X_train, y_train\n",
    "\n",
    "\n",
    "from EEG_Tensorflow_models.Models import DeepConvNet, ShallowConvNet, EEGNet, DMTL_BCI, TCNet_fusion, PST_attention\n",
    "\n",
    "\n",
    "def get_model(model_name, nb_classes):\n",
    "  if model_name == 'DeepConvNet':\n",
    "    model = DeepConvNet\n",
    "    model_params = dict(nb_classes = nb_classes,\n",
    "                      dropoutRate = 0.5, version='2018')\n",
    "    \n",
    "  elif model_name == 'ShallowConvNet':\n",
    "    model = ShallowConvNet\n",
    "    model_params = dict(nb_classes = nb_classes,\n",
    "                      dropoutRate = 0.5,\n",
    "                      version = '2018')\n",
    "    \n",
    "  elif model_name == 'EEGNet':\n",
    "    model = EEGNet\n",
    "    model_params = dict(nb_classes = nb_classes,\n",
    "                      dropoutRate = 0.5,\n",
    "                      kernLength = 32,\n",
    "                      F1 = 8,\n",
    "                      D = 2,\n",
    "                      F2 = 16,\n",
    "                      norm_rate = 0.25, \n",
    "                      dropoutType = 'Dropout')\n",
    "    \n",
    "  elif model_name == 'DMTL_BCI':\n",
    "    model = DMTL_BCI\n",
    "    model_params = dict(nb_classes = nb_classes,\n",
    "                      dropoutRate = 0.5,\n",
    "                      l1 = 0,\n",
    "                      l2 = 0)\n",
    "    \n",
    "  elif model_name == 'TCNet_fusion':\n",
    "    model = TCNet_fusion\n",
    "    model_params = dict(nb_classes = nb_classes,\n",
    "                      layers = 2,\n",
    "                      kernel_s = 4,\n",
    "                      filt = 12,\n",
    "                      dropout = 0.3,\n",
    "                      activation = 'relu',\n",
    "                      F1 = 24,\n",
    "                      D = 2,\n",
    "                      kernLength = 32,\n",
    "                      N_residuals = 2)\n",
    "    \n",
    "  elif model_name == 'PST_attention':\n",
    "    model = PST_attention\n",
    "    model_params = dict(nb_classes = nb_classes,\n",
    "                      dropoutRate = 0.5,\n",
    "                      last_layer = 'Dense')\n",
    "    \n",
    "  else:\n",
    "    raise ValueError('No valid model name')\n",
    "    \n",
    "  return model, model_params\n",
    "\n",
    "from tensorflow.random import set_seed\n",
    "from tensorflow.keras.backend import clear_session\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import accuracy_score, cohen_kappa_score, roc_auc_score,\\\n",
    "                            f1_score, recall_score, precision_score\n",
    "\n",
    "def train(db_name, load_args, cv_args, model_args, compile_args, fit_args, seed):\n",
    "    X_train, y_train = load_DB(db_name, **load_args)\n",
    "    X_train = X_train[..., np.newaxis]  # Add channel dimension\n",
    "    print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "    \n",
    "    cv_results = {'params': [],\n",
    "                  'mean_acc': np.zeros(cv_args['cv'].get_n_splits()),\n",
    "                  'mean_kappa': np.zeros(cv_args['cv'].get_n_splits()),\n",
    "                  'mean_auc': np.zeros(cv_args['cv'].get_n_splits()),\n",
    "                  'mean_f1_left': np.zeros(cv_args['cv'].get_n_splits()),\n",
    "                  'mean_f1_right': np.zeros(cv_args['cv'].get_n_splits()),\n",
    "                  'mean_recall_left': np.zeros(cv_args['cv'].get_n_splits()),\n",
    "                  'mean_recall_right': np.zeros(cv_args['cv'].get_n_splits()),\n",
    "                  'mean_precision_left': np.zeros(cv_args['cv'].get_n_splits()),\n",
    "                  'mean_precision_right': np.zeros(cv_args['cv'].get_n_splits()),\n",
    "                  'all_folds': []}\n",
    "    \n",
    "    k = 0\n",
    "    max_acc = -np.inf\n",
    "\n",
    "    # Loop through folds\n",
    "    for train_index, val_index in cv_args['cv'].split(X_train, y_train):\n",
    "        print(f\"Running fold {k} with {len(train_index)} training samples and {len(val_index)} validation samples\")\n",
    "        print(f\"Fold {k}: train indices: {train_index[:5]}, val indices: {val_index[:5]}\")  # Print first indices\n",
    "        \n",
    "        X, X_val = X_train[train_index], X_train[val_index]\n",
    "        y, y_val = y_train[train_index], y_train[val_index]\n",
    "        \n",
    "        if model_args['autoencoder']:\n",
    "            y = [X, y]\n",
    "        \n",
    "        print(f\"Training data shape: {X.shape}, Validation data shape: {X_val.shape}\")\n",
    "        \n",
    "        batch_size, C, T = X.shape[:-1]\n",
    "        clear_session()\n",
    "        set_seed(seed)\n",
    "\n",
    "        model_cll, model_params = get_model(model_args['model_name'], model_args['nb_classes'])\n",
    "        model = model_cll(**model_params, Chans=64, Samples=T)\n",
    "        model.compile(loss=compile_args['loss'], optimizer=Adam(compile_args['init_lr']))\n",
    "\n",
    "        history = model.fit(X, y, batch_size=batch_size, **fit_args)\n",
    "        print(f\"Fold {k} training loss: {history.history['loss'][-1]}\")  # Print final loss\n",
    "\n",
    "        if model_args['autoencoder']:\n",
    "            y_prob = model.predict(X_val)[-1]\n",
    "        else:\n",
    "            y_prob = model.predict(X_val)\n",
    "        y_pred = np.argmax(y_prob, axis=1)\n",
    "\n",
    "        print(f\"y_true (val): {y_val[:5]}\")\n",
    "        print(f\"y_pred: {y_pred[:5]}\")\n",
    "        \n",
    "        # Calculate metrics\n",
    "        acc = accuracy_score(y_val, y_pred)\n",
    "        kappa = cohen_kappa_score(y_val, y_pred)\n",
    "        auc = roc_auc_score(y_val, y_prob[:, 1], average='macro') if model_args['nb_classes'] == 2 else None\n",
    "        \n",
    "        # Save metrics for this fold\n",
    "        fold_result = {\n",
    "            'fold_index': k,\n",
    "            'train_indices': train_index.tolist(),\n",
    "            'val_indices': val_index.tolist(),\n",
    "            'accuracy': acc,\n",
    "            'kappa': kappa,\n",
    "            'auc': auc\n",
    "        }\n",
    "        print(f\"Appending results for fold {k}: {fold_result}\")\n",
    "        cv_results['all_folds'].append(fold_result)\n",
    "\n",
    "        # Update overall fold metrics\n",
    "        cv_results['mean_acc'][k] = acc\n",
    "        cv_results['mean_kappa'][k] = kappa\n",
    "        if auc is not None:\n",
    "            cv_results['mean_auc'][k] = auc\n",
    "        \n",
    "        # Save the best model weights\n",
    "        if acc > max_acc:\n",
    "            print('New Max Found!')\n",
    "            max_acc = acc\n",
    "            model.save_weights(f'sbj{load_args[\"sbj\"]}.h5')\n",
    "\n",
    "        k += 1\n",
    "    \n",
    "    # Calculate mean and std metrics\n",
    "    cv_results['std_acc'] = round(cv_results['mean_acc'].std(), 3)\n",
    "    cv_results['mean_acc'] = round(cv_results['mean_acc'].mean(), 3)\n",
    "    cv_results['std_kappa'] = round(cv_results['mean_kappa'].std(), 3)\n",
    "    cv_results['mean_kappa'] = round(cv_results['mean_kappa'].mean(), 3)\n",
    "    cv_results['std_auc'] = round(cv_results['mean_auc'].std(), 3)\n",
    "    cv_results['mean_auc'] = round(cv_results['mean_auc'].mean(), 3)\n",
    "    \n",
    "    print(f\"Final cross-validation results: {cv_results}\")\n",
    "    return cv_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ce6ae7",
   "metadata": {
    "id": "uBAeW6J5S68g",
    "papermill": {
     "duration": 0.2423,
     "end_time": "2025-05-21T13:28:30.095903",
     "exception": false,
     "start_time": "2025-05-21T13:28:29.853603",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8999140b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T13:28:30.594553Z",
     "iopub.status.busy": "2025-05-21T13:28:30.593812Z",
     "iopub.status.idle": "2025-05-21T13:28:30.599563Z",
     "shell.execute_reply": "2025-05-21T13:28:30.598262Z"
    },
    "id": "2I3IQnNSS9-a",
    "papermill": {
     "duration": 0.259752,
     "end_time": "2025-05-21T13:28:30.601888",
     "exception": false,
     "start_time": "2025-05-21T13:28:30.342136",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Marcos, use these two variables to run the state of the art. First, for BCICIV2a run all the models.\n",
    "# Remeber that this network DMTL_BCI is an autoencoder. Set the nb_classses parameter depending of the database.\n",
    "# set autoencoder based on the model\n",
    "# We need to run all these tests again. Do not forget to add the recall, preci, and f1 for each class (bci 4, giga 2)\n",
    "db_name = 'GIGA_MI_ME'\n",
    "model_args = dict(model_name = 'TCNet_fusion',\n",
    "                  nb_classes = 2,\n",
    "                  autoencoder = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f282326",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T13:28:31.095381Z",
     "iopub.status.busy": "2025-05-21T13:28:31.094943Z",
     "iopub.status.idle": "2025-05-21T13:28:31.111554Z",
     "shell.execute_reply": "2025-05-21T13:28:31.110102Z"
    },
    "id": "tqMhUFoBIc3B",
    "outputId": "1405fd59-1374-4d5d-8e3a-5e7a45c79bba",
    "papermill": {
     "duration": 0.265825,
     "end_time": "2025-05-21T13:28:31.114215",
     "exception": false,
     "start_time": "2025-05-21T13:28:30.848390",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, TerminateOnNaN\n",
    "import numpy as np\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy, MeanSquaredError\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "if db_name == 'BCICIV2a':\n",
    "  db = Dataset_2a('/kaggle/input/dataset-2a')\n",
    "  fs = db.metadata['sampling_rate']\n",
    "  load_args = dict(db = db,\n",
    "                 fs = fs,\n",
    "                 f_bank = np.asarray([[4., 40.]]),\n",
    "                 vwt = np.asarray([[2.5, 6]]),\n",
    "                 new_fs = 128.)\n",
    "  subjects = np.arange(db.metadata['subjects']) + 1\n",
    "  \n",
    "elif db_name == 'GIGA_MI_ME':\n",
    "  db = GIGA_MI_ME('/kaggle/input/giga-science-gcpds/GIGA_MI_ME')\n",
    "  fs = db.metadata['sampling_rate']\n",
    "\n",
    "  eeg_ch_names = ['Fp1','Fpz','Fp2',\n",
    "                     'AF7','AF3','AFz','AF4','AF8',\n",
    "                    'F7','F5','F3','F1','Fz','F2','F4','F6','F8',\n",
    "                   'FT7','FC5','FC3','FC1','FCz','FC2','FC4','FC6','FT8',\n",
    "                    'T7','C5','C3','C1','Cz','C2','C4','C6','T8',\n",
    "                   'TP7','CP5','CP3','CP1','CPz','CP2','CP4','CP6','TP8',\n",
    "                    'P9','P7','P5','P3','P1','Pz','P2','P4','P6','P8','P10',\n",
    "                    'PO7','PO3','POz','PO4','PO8',\n",
    "                    'O1','Oz','O2',\n",
    "                    'Iz']\n",
    "\n",
    "  # eeg_ch_names = ['Fp1','Fp2',\n",
    "  #                  'AF3','AF4',\n",
    "  #                  'F7','F3','Fz','F4','F8',\n",
    "  #                  'FC5','FC1','FC2','FC6',\n",
    "  #                  'T7','C3','Cz','C4','T8',\n",
    "  #                  'CP5','CP1','CP2','CP6',\n",
    "  #                  'P7','P3','Pz','P4','P8',\n",
    "  #                  'PO3','PO4',\n",
    "  #                  'O1','Oz','O2']\n",
    "\n",
    "  # eeg_ch_names = ['Fp1','Fp2',\n",
    "  #                 'F7','F3','F4','F8',\n",
    "  #                 'T7','C3','C4','T8',\n",
    "  #                 'P7','P3','P4','P8',\n",
    "  #                 'O1','O2']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  # eeg_ch_names = ['Fp1','Fp2',\n",
    "  #              'T7','C3','C4','T8',\n",
    "  #              'O1','O2']\n",
    "    \n",
    "\n",
    "\n",
    "  load_args = dict(db = db,\n",
    "                  eeg_ch_names = eeg_ch_names,\n",
    "                  fs = fs,\n",
    "                  f_bank = np.asarray([[4., 40.]]),\n",
    "                  vwt = np.asarray([[2.5, 5]]),\n",
    "                  new_fs = 128.)\n",
    "  subjects = np.arange(db.metadata['subjects']) + 1\n",
    "  subjects = np.delete(subjects, [28,33])\n",
    "  \n",
    "else:\n",
    "  raise ValueError('No valid database name')\n",
    "\n",
    "verbose = 0\n",
    "reduce_lr_on_plateau = ReduceLROnPlateau(monitor = 'loss', factor = 0.1, patience = 30, verbose = verbose, mode = 'min', min_delta = 0.01, min_lr = 0)\n",
    "terminate_on_nan = TerminateOnNaN()\n",
    "callbacks = [reduce_lr_on_plateau, terminate_on_nan]\n",
    "seed = 23\n",
    "\n",
    "cv_args = dict(cv = StratifiedShuffleSplit(n_splits = 5, test_size = 0.2, random_state = seed))\n",
    "\n",
    "compile_args = dict(loss = SparseCategoricalCrossentropy(), #['mse' , SparseCategoricalCrossentropy()]\n",
    "                    init_lr = 1e-2)\n",
    "                      \n",
    "fit_args = dict(epochs = 500,\n",
    "                verbose = verbose,\n",
    "                callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35065b39",
   "metadata": {
    "id": "ukhXifxzTaj9",
    "papermill": {
     "duration": 0.242757,
     "end_time": "2025-05-21T13:28:31.657840",
     "exception": false,
     "start_time": "2025-05-21T13:28:31.415083",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10f0f1db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T13:28:32.141650Z",
     "iopub.status.busy": "2025-05-21T13:28:32.141258Z",
     "iopub.status.idle": "2025-05-21T16:41:41.568504Z",
     "shell.execute_reply": "2025-05-21T16:41:41.566998Z"
    },
    "id": "Ymqd_W21y3NK",
    "outputId": "5ca97a2f-f57c-46ee-8f53-f00181ccea90",
    "papermill": {
     "duration": 11589.672924,
     "end_time": "2025-05-21T16:41:41.573270",
     "exception": false,
     "start_time": "2025-05-21T13:28:31.900346",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sbj =  9\n",
      "Total trials loaded: 240\n",
      "Shape of X: (240, 64, 3584), Shape of y: (240,)\n",
      "Resampling from 512 to 128.0 Hz.\n",
      "X_train shape: (240, 64, 320, 1), y_train shape: (240,)\n",
      "Running fold 0 with 192 training samples and 48 validation samples\n",
      "Fold 0: train indices: [226 220  34 169  29], val indices: [152  76 228 122  85]\n",
      "Training data shape: (192, 64, 320, 1), Validation data shape: (48, 64, 320, 1)\n",
      "Fold 0 training loss: 0.003059071721509099\n",
      "y_true (val): [1 0 1 1 0]\n",
      "y_pred: [1 0 0 1 1]\n",
      "Appending results for fold 0: {'fold_index': 0, 'train_indices': [226, 220, 34, 169, 29, 161, 156, 60, 159, 48, 223, 9, 59, 155, 119, 173, 46, 68, 33, 56, 19, 5, 195, 97, 208, 24, 149, 126, 205, 53, 157, 41, 234, 112, 30, 132, 187, 213, 212, 71, 38, 61, 106, 58, 100, 108, 18, 130, 170, 77, 207, 199, 214, 86, 26, 47, 197, 183, 136, 22, 204, 158, 127, 148, 116, 177, 80, 140, 224, 50, 95, 70, 101, 133, 139, 160, 227, 184, 137, 201, 96, 92, 16, 125, 217, 65, 82, 87, 146, 232, 28, 215, 150, 57, 88, 99, 166, 196, 81, 238, 43, 17, 55, 93, 52, 21, 237, 98, 165, 35, 128, 178, 84, 189, 153, 78, 141, 210, 229, 185, 179, 193, 89, 138, 14, 11, 164, 107, 3, 110, 23, 191, 225, 62, 37, 124, 123, 74, 15, 2, 131, 8, 145, 63, 211, 194, 180, 44, 105, 1, 115, 32, 143, 20, 174, 118, 167, 36, 203, 113, 171, 134, 219, 216, 233, 188, 144, 13, 221, 72, 111, 202, 94, 147, 114, 10, 235, 175, 4, 42, 79, 67, 218, 0, 239, 192, 27, 117, 200, 172, 7, 206], 'val_indices': [152, 76, 228, 122, 85, 103, 31, 120, 102, 222, 209, 129, 163, 142, 75, 49, 45, 69, 151, 73, 236, 83, 51, 135, 6, 91, 154, 231, 186, 104, 182, 66, 12, 168, 40, 190, 90, 181, 230, 54, 25, 176, 109, 162, 121, 39, 198, 64], 'accuracy': 0.625, 'kappa': 0.25, 'auc': 0.6631944444444444}\n",
      "New Max Found!\n",
      "Running fold 1 with 192 training samples and 48 validation samples\n",
      "Fold 1: train indices: [132 148 150 159  76], val indices: [238 116  61  84  44]\n",
      "Training data shape: (192, 64, 320, 1), Validation data shape: (48, 64, 320, 1)\n",
      "Fold 1 training loss: 0.005513748619705439\n",
      "y_true (val): [1 0 0 0 0]\n",
      "y_pred: [1 0 0 0 1]\n",
      "Appending results for fold 1: {'fold_index': 1, 'train_indices': [132, 148, 150, 159, 76, 70, 120, 117, 47, 129, 166, 18, 177, 154, 203, 111, 135, 176, 160, 34, 19, 130, 126, 98, 191, 193, 68, 119, 25, 196, 163, 13, 91, 26, 136, 121, 78, 174, 199, 179, 36, 198, 63, 133, 134, 210, 170, 185, 218, 138, 55, 145, 49, 59, 205, 94, 102, 115, 234, 57, 208, 165, 142, 28, 144, 69, 124, 82, 95, 40, 7, 131, 188, 226, 37, 80, 97, 180, 20, 214, 223, 112, 173, 96, 220, 110, 12, 88, 17, 22, 106, 101, 233, 109, 21, 182, 137, 236, 66, 6, 90, 228, 30, 194, 79, 10, 215, 43, 62, 113, 224, 5, 99, 161, 16, 147, 45, 2, 230, 8, 118, 81, 38, 32, 14, 227, 87, 195, 213, 122, 93, 52, 108, 237, 200, 128, 222, 67, 178, 114, 83, 181, 54, 31, 183, 73, 3, 29, 164, 60, 42, 39, 190, 72, 221, 219, 216, 58, 153, 89, 125, 202, 50, 64, 23, 74, 162, 158, 189, 4, 235, 92, 187, 123, 211, 141, 239, 65, 149, 27, 231, 217, 75, 139, 46, 143, 85, 146, 204, 197, 0, 151], 'val_indices': [238, 116, 61, 84, 44, 168, 107, 184, 86, 207, 201, 156, 209, 225, 1, 232, 15, 48, 35, 175, 206, 77, 186, 11, 152, 71, 157, 140, 33, 104, 24, 53, 9, 171, 100, 127, 155, 192, 229, 105, 103, 41, 51, 167, 56, 212, 172, 169], 'accuracy': 0.7083333333333334, 'kappa': 0.41666666666666663, 'auc': 0.796875}\n",
      "New Max Found!\n",
      "Running fold 2 with 192 training samples and 48 validation samples\n",
      "Fold 2: train indices: [168 214 118 197 149], val indices: [134  50 170 234  58]\n",
      "Training data shape: (192, 64, 320, 1), Validation data shape: (48, 64, 320, 1)\n",
      "Fold 2 training loss: 0.009920620359480381\n",
      "y_true (val): [1 0 1 1 0]\n",
      "y_pred: [1 1 0 1 0]\n",
      "Appending results for fold 2: {'fold_index': 2, 'train_indices': [168, 214, 118, 197, 149, 72, 117, 125, 44, 46, 225, 8, 142, 49, 108, 24, 237, 178, 123, 192, 92, 66, 164, 238, 32, 27, 190, 88, 105, 16, 186, 205, 124, 45, 90, 151, 155, 141, 204, 14, 37, 235, 96, 189, 126, 102, 231, 103, 161, 22, 75, 121, 25, 146, 110, 210, 84, 221, 211, 93, 209, 71, 140, 81, 112, 115, 227, 167, 65, 207, 199, 153, 3, 145, 74, 85, 76, 233, 48, 95, 98, 18, 87, 198, 89, 129, 53, 148, 47, 60, 182, 31, 232, 56, 188, 203, 26, 206, 212, 181, 201, 179, 39, 187, 158, 156, 154, 175, 144, 6, 55, 62, 229, 69, 9, 174, 176, 61, 86, 104, 163, 183, 162, 191, 40, 35, 68, 78, 195, 64, 12, 200, 208, 152, 42, 101, 138, 54, 196, 222, 83, 184, 113, 169, 193, 17, 239, 109, 147, 13, 63, 173, 29, 135, 185, 122, 150, 77, 137, 97, 157, 119, 133, 166, 94, 11, 5, 218, 59, 132, 0, 10, 91, 131, 20, 180, 216, 107, 4, 213, 171, 80, 99, 51, 127, 236, 111, 23, 70, 136, 7, 116], 'val_indices': [134, 50, 170, 234, 58, 28, 79, 52, 215, 1, 21, 160, 100, 67, 230, 159, 177, 202, 106, 15, 43, 220, 139, 57, 224, 223, 219, 226, 172, 165, 228, 120, 194, 38, 36, 82, 128, 143, 34, 114, 33, 217, 19, 2, 130, 30, 73, 41], 'accuracy': 0.75, 'kappa': 0.5, 'auc': 0.7899305555555556}\n",
      "New Max Found!\n",
      "Running fold 3 with 192 training samples and 48 validation samples\n",
      "Fold 3: train indices: [219 224 196 126 171], val indices: [175 117 237  59 216]\n",
      "Training data shape: (192, 64, 320, 1), Validation data shape: (48, 64, 320, 1)\n",
      "Fold 3 training loss: 0.009371605701744556\n",
      "y_true (val): [1 0 1 0 1]\n",
      "y_pred: [1 0 1 1 1]\n",
      "Appending results for fold 3: {'fold_index': 3, 'train_indices': [219, 224, 196, 126, 171, 81, 203, 107, 173, 142, 64, 10, 66, 152, 166, 29, 210, 78, 52, 63, 28, 223, 211, 23, 215, 164, 220, 188, 185, 12, 4, 170, 21, 103, 95, 55, 97, 217, 3, 98, 19, 83, 53, 123, 116, 84, 145, 194, 184, 2, 58, 206, 214, 94, 121, 51, 168, 42, 212, 226, 150, 238, 108, 118, 43, 197, 35, 156, 136, 193, 27, 87, 228, 162, 179, 62, 141, 221, 130, 47, 191, 88, 18, 69, 192, 26, 80, 233, 71, 176, 15, 151, 16, 0, 17, 236, 198, 73, 177, 32, 202, 41, 25, 13, 138, 60, 182, 92, 11, 144, 46, 77, 163, 111, 93, 234, 44, 201, 135, 207, 209, 161, 190, 70, 199, 119, 189, 114, 56, 127, 112, 45, 120, 30, 129, 178, 186, 106, 39, 9, 5, 24, 205, 139, 74, 105, 158, 222, 133, 72, 124, 22, 195, 174, 20, 65, 149, 8, 100, 132, 208, 96, 230, 101, 128, 90, 146, 57, 137, 160, 165, 7, 61, 125, 231, 109, 204, 131, 38, 122, 229, 91, 40, 54, 6, 37, 218, 31, 143, 200, 153, 36], 'val_indices': [175, 117, 237, 59, 216, 225, 75, 172, 82, 79, 76, 99, 48, 134, 14, 147, 232, 68, 169, 187, 102, 239, 235, 49, 148, 167, 104, 85, 180, 227, 33, 154, 113, 183, 67, 155, 159, 50, 213, 86, 140, 110, 89, 115, 157, 181, 34, 1], 'accuracy': 0.7291666666666666, 'kappa': 0.45833333333333337, 'auc': 0.8333333333333334}\n",
      "Running fold 4 with 192 training samples and 48 validation samples\n",
      "Fold 4: train indices: [182 140 177  57 197], val indices: [144  66  63  73 206]\n",
      "Training data shape: (192, 64, 320, 1), Validation data shape: (48, 64, 320, 1)\n",
      "Fold 4 training loss: 0.0035598475951701403\n",
      "y_true (val): [1 0 0 0 1]\n",
      "y_pred: [0 0 0 0 1]\n",
      "Appending results for fold 4: {'fold_index': 4, 'train_indices': [182, 140, 177, 57, 197, 111, 45, 24, 215, 68, 32, 116, 59, 130, 17, 105, 165, 19, 219, 41, 97, 14, 72, 107, 20, 180, 64, 33, 118, 231, 95, 91, 224, 10, 30, 71, 22, 151, 3, 159, 69, 191, 83, 200, 141, 148, 18, 27, 127, 167, 7, 106, 61, 237, 172, 43, 82, 121, 160, 81, 149, 37, 222, 210, 109, 205, 188, 44, 239, 76, 162, 199, 15, 47, 42, 110, 29, 204, 75, 135, 128, 0, 123, 70, 56, 77, 119, 49, 235, 90, 60, 184, 53, 232, 103, 220, 54, 234, 26, 176, 169, 229, 233, 122, 94, 88, 194, 96, 115, 198, 65, 78, 145, 134, 171, 34, 166, 38, 139, 227, 98, 133, 163, 201, 117, 100, 92, 55, 187, 28, 104, 85, 138, 89, 6, 25, 178, 203, 208, 174, 136, 168, 142, 150, 1, 154, 51, 153, 13, 39, 16, 238, 126, 120, 236, 202, 35, 164, 9, 193, 156, 11, 146, 155, 217, 79, 125, 86, 183, 87, 102, 179, 131, 221, 212, 31, 189, 230, 185, 99, 147, 157, 158, 137, 226, 80, 228, 50, 108, 196, 124, 113], 'val_indices': [144, 66, 63, 73, 206, 74, 192, 2, 213, 129, 132, 152, 84, 93, 173, 195, 101, 4, 214, 5, 223, 218, 211, 36, 21, 170, 23, 114, 112, 67, 175, 40, 190, 207, 181, 8, 12, 62, 161, 143, 48, 46, 186, 225, 209, 216, 58, 52], 'accuracy': 0.6666666666666666, 'kappa': 0.33333333333333337, 'auc': 0.7586805555555556}\n",
      "Final cross-validation results: {'params': [], 'mean_acc': 0.696, 'mean_kappa': 0.392, 'mean_auc': 0.768, 'mean_f1_left': array([0., 0., 0., 0., 0.]), 'mean_f1_right': array([0., 0., 0., 0., 0.]), 'mean_recall_left': array([0., 0., 0., 0., 0.]), 'mean_recall_right': array([0., 0., 0., 0., 0.]), 'mean_precision_left': array([0., 0., 0., 0., 0.]), 'mean_precision_right': array([0., 0., 0., 0., 0.]), 'all_folds': [{'fold_index': 0, 'train_indices': [226, 220, 34, 169, 29, 161, 156, 60, 159, 48, 223, 9, 59, 155, 119, 173, 46, 68, 33, 56, 19, 5, 195, 97, 208, 24, 149, 126, 205, 53, 157, 41, 234, 112, 30, 132, 187, 213, 212, 71, 38, 61, 106, 58, 100, 108, 18, 130, 170, 77, 207, 199, 214, 86, 26, 47, 197, 183, 136, 22, 204, 158, 127, 148, 116, 177, 80, 140, 224, 50, 95, 70, 101, 133, 139, 160, 227, 184, 137, 201, 96, 92, 16, 125, 217, 65, 82, 87, 146, 232, 28, 215, 150, 57, 88, 99, 166, 196, 81, 238, 43, 17, 55, 93, 52, 21, 237, 98, 165, 35, 128, 178, 84, 189, 153, 78, 141, 210, 229, 185, 179, 193, 89, 138, 14, 11, 164, 107, 3, 110, 23, 191, 225, 62, 37, 124, 123, 74, 15, 2, 131, 8, 145, 63, 211, 194, 180, 44, 105, 1, 115, 32, 143, 20, 174, 118, 167, 36, 203, 113, 171, 134, 219, 216, 233, 188, 144, 13, 221, 72, 111, 202, 94, 147, 114, 10, 235, 175, 4, 42, 79, 67, 218, 0, 239, 192, 27, 117, 200, 172, 7, 206], 'val_indices': [152, 76, 228, 122, 85, 103, 31, 120, 102, 222, 209, 129, 163, 142, 75, 49, 45, 69, 151, 73, 236, 83, 51, 135, 6, 91, 154, 231, 186, 104, 182, 66, 12, 168, 40, 190, 90, 181, 230, 54, 25, 176, 109, 162, 121, 39, 198, 64], 'accuracy': 0.625, 'kappa': 0.25, 'auc': 0.6631944444444444}, {'fold_index': 1, 'train_indices': [132, 148, 150, 159, 76, 70, 120, 117, 47, 129, 166, 18, 177, 154, 203, 111, 135, 176, 160, 34, 19, 130, 126, 98, 191, 193, 68, 119, 25, 196, 163, 13, 91, 26, 136, 121, 78, 174, 199, 179, 36, 198, 63, 133, 134, 210, 170, 185, 218, 138, 55, 145, 49, 59, 205, 94, 102, 115, 234, 57, 208, 165, 142, 28, 144, 69, 124, 82, 95, 40, 7, 131, 188, 226, 37, 80, 97, 180, 20, 214, 223, 112, 173, 96, 220, 110, 12, 88, 17, 22, 106, 101, 233, 109, 21, 182, 137, 236, 66, 6, 90, 228, 30, 194, 79, 10, 215, 43, 62, 113, 224, 5, 99, 161, 16, 147, 45, 2, 230, 8, 118, 81, 38, 32, 14, 227, 87, 195, 213, 122, 93, 52, 108, 237, 200, 128, 222, 67, 178, 114, 83, 181, 54, 31, 183, 73, 3, 29, 164, 60, 42, 39, 190, 72, 221, 219, 216, 58, 153, 89, 125, 202, 50, 64, 23, 74, 162, 158, 189, 4, 235, 92, 187, 123, 211, 141, 239, 65, 149, 27, 231, 217, 75, 139, 46, 143, 85, 146, 204, 197, 0, 151], 'val_indices': [238, 116, 61, 84, 44, 168, 107, 184, 86, 207, 201, 156, 209, 225, 1, 232, 15, 48, 35, 175, 206, 77, 186, 11, 152, 71, 157, 140, 33, 104, 24, 53, 9, 171, 100, 127, 155, 192, 229, 105, 103, 41, 51, 167, 56, 212, 172, 169], 'accuracy': 0.7083333333333334, 'kappa': 0.41666666666666663, 'auc': 0.796875}, {'fold_index': 2, 'train_indices': [168, 214, 118, 197, 149, 72, 117, 125, 44, 46, 225, 8, 142, 49, 108, 24, 237, 178, 123, 192, 92, 66, 164, 238, 32, 27, 190, 88, 105, 16, 186, 205, 124, 45, 90, 151, 155, 141, 204, 14, 37, 235, 96, 189, 126, 102, 231, 103, 161, 22, 75, 121, 25, 146, 110, 210, 84, 221, 211, 93, 209, 71, 140, 81, 112, 115, 227, 167, 65, 207, 199, 153, 3, 145, 74, 85, 76, 233, 48, 95, 98, 18, 87, 198, 89, 129, 53, 148, 47, 60, 182, 31, 232, 56, 188, 203, 26, 206, 212, 181, 201, 179, 39, 187, 158, 156, 154, 175, 144, 6, 55, 62, 229, 69, 9, 174, 176, 61, 86, 104, 163, 183, 162, 191, 40, 35, 68, 78, 195, 64, 12, 200, 208, 152, 42, 101, 138, 54, 196, 222, 83, 184, 113, 169, 193, 17, 239, 109, 147, 13, 63, 173, 29, 135, 185, 122, 150, 77, 137, 97, 157, 119, 133, 166, 94, 11, 5, 218, 59, 132, 0, 10, 91, 131, 20, 180, 216, 107, 4, 213, 171, 80, 99, 51, 127, 236, 111, 23, 70, 136, 7, 116], 'val_indices': [134, 50, 170, 234, 58, 28, 79, 52, 215, 1, 21, 160, 100, 67, 230, 159, 177, 202, 106, 15, 43, 220, 139, 57, 224, 223, 219, 226, 172, 165, 228, 120, 194, 38, 36, 82, 128, 143, 34, 114, 33, 217, 19, 2, 130, 30, 73, 41], 'accuracy': 0.75, 'kappa': 0.5, 'auc': 0.7899305555555556}, {'fold_index': 3, 'train_indices': [219, 224, 196, 126, 171, 81, 203, 107, 173, 142, 64, 10, 66, 152, 166, 29, 210, 78, 52, 63, 28, 223, 211, 23, 215, 164, 220, 188, 185, 12, 4, 170, 21, 103, 95, 55, 97, 217, 3, 98, 19, 83, 53, 123, 116, 84, 145, 194, 184, 2, 58, 206, 214, 94, 121, 51, 168, 42, 212, 226, 150, 238, 108, 118, 43, 197, 35, 156, 136, 193, 27, 87, 228, 162, 179, 62, 141, 221, 130, 47, 191, 88, 18, 69, 192, 26, 80, 233, 71, 176, 15, 151, 16, 0, 17, 236, 198, 73, 177, 32, 202, 41, 25, 13, 138, 60, 182, 92, 11, 144, 46, 77, 163, 111, 93, 234, 44, 201, 135, 207, 209, 161, 190, 70, 199, 119, 189, 114, 56, 127, 112, 45, 120, 30, 129, 178, 186, 106, 39, 9, 5, 24, 205, 139, 74, 105, 158, 222, 133, 72, 124, 22, 195, 174, 20, 65, 149, 8, 100, 132, 208, 96, 230, 101, 128, 90, 146, 57, 137, 160, 165, 7, 61, 125, 231, 109, 204, 131, 38, 122, 229, 91, 40, 54, 6, 37, 218, 31, 143, 200, 153, 36], 'val_indices': [175, 117, 237, 59, 216, 225, 75, 172, 82, 79, 76, 99, 48, 134, 14, 147, 232, 68, 169, 187, 102, 239, 235, 49, 148, 167, 104, 85, 180, 227, 33, 154, 113, 183, 67, 155, 159, 50, 213, 86, 140, 110, 89, 115, 157, 181, 34, 1], 'accuracy': 0.7291666666666666, 'kappa': 0.45833333333333337, 'auc': 0.8333333333333334}, {'fold_index': 4, 'train_indices': [182, 140, 177, 57, 197, 111, 45, 24, 215, 68, 32, 116, 59, 130, 17, 105, 165, 19, 219, 41, 97, 14, 72, 107, 20, 180, 64, 33, 118, 231, 95, 91, 224, 10, 30, 71, 22, 151, 3, 159, 69, 191, 83, 200, 141, 148, 18, 27, 127, 167, 7, 106, 61, 237, 172, 43, 82, 121, 160, 81, 149, 37, 222, 210, 109, 205, 188, 44, 239, 76, 162, 199, 15, 47, 42, 110, 29, 204, 75, 135, 128, 0, 123, 70, 56, 77, 119, 49, 235, 90, 60, 184, 53, 232, 103, 220, 54, 234, 26, 176, 169, 229, 233, 122, 94, 88, 194, 96, 115, 198, 65, 78, 145, 134, 171, 34, 166, 38, 139, 227, 98, 133, 163, 201, 117, 100, 92, 55, 187, 28, 104, 85, 138, 89, 6, 25, 178, 203, 208, 174, 136, 168, 142, 150, 1, 154, 51, 153, 13, 39, 16, 238, 126, 120, 236, 202, 35, 164, 9, 193, 156, 11, 146, 155, 217, 79, 125, 86, 183, 87, 102, 179, 131, 221, 212, 31, 189, 230, 185, 99, 147, 157, 158, 137, 226, 80, 228, 50, 108, 196, 124, 113], 'val_indices': [144, 66, 63, 73, 206, 74, 192, 2, 213, 129, 132, 152, 84, 93, 173, 195, 101, 4, 214, 5, 223, 218, 211, 36, 21, 170, 23, 114, 112, 67, 175, 40, 190, 207, 181, 8, 12, 62, 161, 143, 48, 46, 186, 225, 209, 216, 58, 52], 'accuracy': 0.6666666666666666, 'kappa': 0.33333333333333337, 'auc': 0.7586805555555556}], 'std_acc': 0.045, 'std_kappa': 0.09, 'std_auc': 0.058}\n"
     ]
    }
   ],
   "source": [
    "from pickle import dump\n",
    "\n",
    "subjects = [9]\n",
    "\n",
    "for sbj in subjects[:]:\n",
    "  print('sbj = ', sbj)\n",
    "  load_args['sbj'] = sbj\n",
    "  results = train(db_name, load_args, cv_args, model_args, compile_args, fit_args, seed)\n",
    "  with open('sbj' + str(load_args['sbj']) + '.txt', 'wb') as f:\n",
    "    dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76c0d013",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T16:41:42.353252Z",
     "iopub.status.busy": "2025-05-21T16:41:42.352784Z",
     "iopub.status.idle": "2025-05-21T16:41:44.649598Z",
     "shell.execute_reply": "2025-05-21T16:41:44.648240Z"
    },
    "id": "V7-P0xjwzXVX",
    "outputId": "270dceef-351d-48d1-f71e-2c3367c7fdac",
    "papermill": {
     "duration": 2.835701,
     "end_time": "2025-05-21T16:41:44.652580",
     "exception": false,
     "start_time": "2025-05-21T16:41:41.816879",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: sbj9.h5 (deflated 65%)\r\n",
      "  adding: sbj9.txt (deflated 41%)\r\n"
     ]
    }
   ],
   "source": [
    "!zip Models_64ch_TCNet.zip ./*.h5 \n",
    "!zip Results_64ch_TCNet.zip ./*.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d88bbf64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T16:41:45.143693Z",
     "iopub.status.busy": "2025-05-21T16:41:45.143226Z",
     "iopub.status.idle": "2025-05-21T16:41:45.148893Z",
     "shell.execute_reply": "2025-05-21T16:41:45.147716Z"
    },
    "papermill": {
     "duration": 0.256138,
     "end_time": "2025-05-21T16:41:45.151142",
     "exception": false,
     "start_time": "2025-05-21T16:41:44.895004",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import pickle as pkl\n",
    "\n",
    "#with open(file= '/kaggle/working/sbj14.txt', mode = 'rb' ) as f:\n",
    "#    results_64ch_ShallowConvNet = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71071fef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T16:41:45.645533Z",
     "iopub.status.busy": "2025-05-21T16:41:45.644612Z",
     "iopub.status.idle": "2025-05-21T16:41:45.650512Z",
     "shell.execute_reply": "2025-05-21T16:41:45.649349Z"
    },
    "papermill": {
     "duration": 0.256295,
     "end_time": "2025-05-21T16:41:45.652613",
     "exception": false,
     "start_time": "2025-05-21T16:41:45.396318",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#results_64ch_ShallowConvNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38cd39f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T16:41:46.195379Z",
     "iopub.status.busy": "2025-05-21T16:41:46.194826Z",
     "iopub.status.idle": "2025-05-21T16:41:46.200011Z",
     "shell.execute_reply": "2025-05-21T16:41:46.198638Z"
    },
    "papermill": {
     "duration": 0.252656,
     "end_time": "2025-05-21T16:41:46.202188",
     "exception": false,
     "start_time": "2025-05-21T16:41:45.949532",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#with open(file= '/kaggle/working/sbj2.txt', mode = 'rb' ) as f:\n",
    " #   results_64ch_ShallowConvNet = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f0065c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T16:41:46.699459Z",
     "iopub.status.busy": "2025-05-21T16:41:46.698999Z",
     "iopub.status.idle": "2025-05-21T16:41:46.704387Z",
     "shell.execute_reply": "2025-05-21T16:41:46.703048Z"
    },
    "papermill": {
     "duration": 0.260174,
     "end_time": "2025-05-21T16:41:46.706414",
     "exception": false,
     "start_time": "2025-05-21T16:41:46.446240",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#results_64ch_ShallowConvNet"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 1645904,
     "sourceId": 2702213,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 1269900,
     "sourceId": 2702226,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 2984453,
     "sourceId": 5137200,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3008205,
     "sourceId": 5175158,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 11984.019245,
   "end_time": "2025-05-21T16:41:50.286627",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-21T13:22:06.267382",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
